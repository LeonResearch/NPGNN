{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from model import GNP_Encoder, GNP_Decoder, InnerProductDecoder\n",
    "from optimizer import loss_function3\n",
    "from utils import load_data, mask_test_edges, preprocess_graph, get_roc_score, ct_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1679c7abb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, default='gcn_vae', help=\"models used\")\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500, help='Number of epochs to train.')\n",
    "parser.add_argument('--hiddenEnc', type=int, default=32, help='Number of units in hidden layer of Encoder.')\n",
    "parser.add_argument('--z_dim', type=int, default=32, help='Dimension of latent code Z.')\n",
    "parser.add_argument('--hiddenDec', type=int, default=64, help='Number of units in hidden layer of Decoder.')\n",
    "parser.add_argument('--outDimDec', type=int, default=32, help='Output Dimension of the Decoder.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset_str', type=str, default='cora', help='type of dataset.')\n",
    "parser.add_argument('--n_z_samples', type=int, default=10, help='Number of Z samples')\n",
    "\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, std, n):\n",
    "    \"\"\"Reparameterisation trick.\"\"\"\n",
    "    eps = torch.autograd.Variable(std.data.new(n,args.z_dim).normal_())\n",
    "    return mu + std * eps \n",
    "\n",
    "def KLD_gaussian(mu_q, std_q, mu_p, std_p):\n",
    "    \"\"\"Analytical KLD between 2 Gaussians.\"\"\"\n",
    "    qs2 = std_q**2 + 1e-16\n",
    "    ps2 = std_p**2 + 1e-16\n",
    "    \n",
    "    return (qs2/ps2 + ((mu_q-mu_p)**2)/ps2 + torch.log(ps2/qs2) - 1.0).sum()*0.5\n",
    "    \n",
    "def gae_for(args,if_plot):\n",
    "    adj, features = load_data(args.dataset_str)\n",
    "    n_nodes, feat_dim, = features.shape\n",
    "    features = features.to(device)\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))\n",
    "    \n",
    "    \n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    adj_deep_copy = adj\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)   \n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    encoder = GNP_Encoder(feat_dim, args.hiddenEnc, args.z_dim, args.dropout)\n",
    "    decoder = GNP_Decoder(feat_dim + args.z_dim, args.hiddenDec, args.outDimDec, args.dropout)\n",
    "    innerDecoder = InnerProductDecoder(args.dropout, act=lambda x: x)\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    innerDecoder.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(decoder.parameters())+list(encoder.parameters()), args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_ap = []\n",
    "    for epoch in range(args.epochs):\n",
    "        \n",
    "        \n",
    "        t = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        innerDecoder.train()\n",
    "        \n",
    "        \n",
    "        np.random.seed(epoch)\n",
    "        adj_context  = ct_split(adj_deep_copy )\n",
    "        adj_context_norm = preprocess_graph(adj_context)\n",
    "        adj_context_norm = adj_context_norm.to(device)\n",
    "        \n",
    "        \n",
    "        c_z_mu, c_z_logvar = encoder(features, adj_context_norm)\n",
    "        ct_z_mu, ct_z_logvar = encoder(features, adj_norm)\n",
    "        \n",
    "        #Sample a batch of zs using reparam trick for MC estimation\n",
    "        zs = sample_z(ct_z_mu, torch.exp(ct_z_logvar), args.n_z_samples)\n",
    "        zs = zs.to(device)   \n",
    "\n",
    "        # Get the predictive distribution of y*\n",
    "        mu, std = decoder(features, zs)\n",
    "\n",
    "        emb = torch.mean(mu, dim = 1)\n",
    "        pred_adj = innerDecoder(emb)\n",
    "        \n",
    "        #Compute loss and backprop\n",
    "        loss = loss_function3(preds=pred_adj, labels=adj_label, norm=norm, pos_weight=torch.tensor(pos_weight))  + KLD_gaussian(ct_z_mu, torch.exp(ct_z_logvar), c_z_mu, torch.exp(c_z_logvar))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()        \n",
    "        scheduler.step()\n",
    "        \n",
    "        hidden_emb = emb.cpu().data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "        \n",
    "        \n",
    "        train_loss.append(cur_loss)\n",
    "        val_ap.append(ap_curr)\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "              )\n",
    "\n",
    "        if (if_plot == True) and (epoch%49 == 0):\n",
    "            fig = plt.figure(figsize=(30,10))\n",
    "            ax1 = fig.add_subplot(1,2,1)\n",
    "            ax2 = fig.add_subplot(1,2,2)\n",
    "            \n",
    "            ax1.plot(train_loss, label='Training loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend(frameon=False)\n",
    "    \n",
    "            ax2.plot(val_ap, label='Validation Average Precision Score',color='Red')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('AP')\n",
    "            ax2.legend(frameon=False)\n",
    "            \n",
    "            plt.show()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using cora dataset\n",
      "Epoch: 0001 train_loss= 6.38497 val_ap= 0.50947 time= 0.51023\n",
      "Epoch: 0002 train_loss= 1.87491 val_ap= 0.52450 time= 0.08577\n",
      "Epoch: 0003 train_loss= 0.85954 val_ap= 0.55594 time= 0.07679\n",
      "Epoch: 0004 train_loss= 0.94885 val_ap= 0.54348 time= 0.09774\n",
      "Epoch: 0005 train_loss= 1.22695 val_ap= 0.54390 time= 0.07480\n",
      "Epoch: 0006 train_loss= 1.24099 val_ap= 0.54373 time= 0.07779\n",
      "Epoch: 0007 train_loss= 1.07747 val_ap= 0.53466 time= 0.08577\n",
      "Epoch: 0008 train_loss= 0.97753 val_ap= 0.51889 time= 0.07679\n",
      "Epoch: 0009 train_loss= 0.92885 val_ap= 0.51553 time= 0.07979\n",
      "Epoch: 0010 train_loss= 0.83266 val_ap= 0.51335 time= 0.07879\n",
      "Epoch: 0011 train_loss= 0.81389 val_ap= 0.52249 time= 0.08677\n",
      "Epoch: 0012 train_loss= 0.80165 val_ap= 0.54310 time= 0.08278\n",
      "Epoch: 0013 train_loss= 0.79805 val_ap= 0.55640 time= 0.09375\n",
      "Epoch: 0014 train_loss= 0.79643 val_ap= 0.58052 time= 0.08178\n",
      "Epoch: 0015 train_loss= 0.79125 val_ap= 0.58110 time= 0.07081\n",
      "Epoch: 0016 train_loss= 0.78987 val_ap= 0.59505 time= 0.08677\n",
      "Epoch: 0017 train_loss= 0.78389 val_ap= 0.57566 time= 0.07680\n",
      "Epoch: 0018 train_loss= 0.78325 val_ap= 0.59756 time= 0.08477\n",
      "Epoch: 0019 train_loss= 0.78175 val_ap= 0.59794 time= 0.07877\n",
      "Epoch: 0020 train_loss= 0.78117 val_ap= 0.61379 time= 0.07580\n",
      "Epoch: 0021 train_loss= 0.78162 val_ap= 0.61749 time= 0.07580\n",
      "Epoch: 0022 train_loss= 0.78049 val_ap= 0.59544 time= 0.08577\n",
      "Epoch: 0023 train_loss= 0.78044 val_ap= 0.59186 time= 0.09076\n",
      "Epoch: 0024 train_loss= 0.77816 val_ap= 0.62747 time= 0.07680\n",
      "Epoch: 0025 train_loss= 0.77813 val_ap= 0.61640 time= 0.07979\n",
      "Epoch: 0026 train_loss= 0.77739 val_ap= 0.64924 time= 0.07779\n",
      "Epoch: 0027 train_loss= 0.77609 val_ap= 0.63761 time= 0.07281\n",
      "Epoch: 0028 train_loss= 0.77625 val_ap= 0.62323 time= 0.07679\n",
      "Epoch: 0029 train_loss= 0.77523 val_ap= 0.63338 time= 0.08278\n",
      "Epoch: 0030 train_loss= 0.77666 val_ap= 0.62769 time= 0.08178\n",
      "Epoch: 0031 train_loss= 0.77291 val_ap= 0.65347 time= 0.07578\n",
      "Epoch: 0032 train_loss= 0.77268 val_ap= 0.64016 time= 0.07779\n",
      "Epoch: 0033 train_loss= 0.76959 val_ap= 0.65651 time= 0.08378\n",
      "Epoch: 0034 train_loss= 0.77153 val_ap= 0.66263 time= 0.08377\n",
      "Epoch: 0035 train_loss= 0.76844 val_ap= 0.66211 time= 0.08477\n",
      "Epoch: 0036 train_loss= 0.77420 val_ap= 0.66175 time= 0.08178\n",
      "Epoch: 0037 train_loss= 0.76553 val_ap= 0.66196 time= 0.08577\n",
      "Epoch: 0038 train_loss= 0.76365 val_ap= 0.65990 time= 0.08178\n",
      "Epoch: 0039 train_loss= 0.76267 val_ap= 0.66115 time= 0.07779\n",
      "Epoch: 0040 train_loss= 0.76426 val_ap= 0.65969 time= 0.08677\n",
      "Epoch: 0041 train_loss= 0.76875 val_ap= 0.64968 time= 0.07680\n",
      "Epoch: 0042 train_loss= 0.76068 val_ap= 0.66233 time= 0.07878\n",
      "Epoch: 0043 train_loss= 0.76478 val_ap= 0.65551 time= 0.08078\n",
      "Epoch: 0044 train_loss= 0.75843 val_ap= 0.65817 time= 0.09475\n",
      "Epoch: 0045 train_loss= 0.75611 val_ap= 0.65804 time= 0.08577\n",
      "Epoch: 0046 train_loss= 0.75437 val_ap= 0.65643 time= 0.08378\n",
      "Epoch: 0047 train_loss= 0.75198 val_ap= 0.65821 time= 0.07879\n",
      "Epoch: 0048 train_loss= 0.75153 val_ap= 0.65669 time= 0.07580\n",
      "Epoch: 0049 train_loss= 0.74971 val_ap= 0.65834 time= 0.07580\n",
      "Epoch: 0050 train_loss= 0.77299 val_ap= 0.66033 time= 0.07580\n",
      "Epoch: 0051 train_loss= 0.75298 val_ap= 0.66036 time= 0.08477\n",
      "Epoch: 0052 train_loss= 0.74859 val_ap= 0.65504 time= 0.07879\n",
      "Epoch: 0053 train_loss= 0.74481 val_ap= 0.65784 time= 0.07580\n",
      "Epoch: 0054 train_loss= 0.74205 val_ap= 0.65972 time= 0.09176\n",
      "Epoch: 0055 train_loss= 0.74026 val_ap= 0.66179 time= 0.07879\n",
      "Epoch: 0056 train_loss= 0.74319 val_ap= 0.65630 time= 0.07979\n",
      "Epoch: 0057 train_loss= 0.73775 val_ap= 0.65972 time= 0.08876\n",
      "Epoch: 0058 train_loss= 0.73788 val_ap= 0.65820 time= 0.08178\n",
      "Epoch: 0059 train_loss= 0.73516 val_ap= 0.65999 time= 0.08776\n",
      "Epoch: 0060 train_loss= 0.73402 val_ap= 0.66128 time= 0.08677\n",
      "Epoch: 0061 train_loss= 0.73211 val_ap= 0.65447 time= 0.09275\n",
      "Epoch: 0062 train_loss= 0.73174 val_ap= 0.66041 time= 0.08577\n",
      "Epoch: 0063 train_loss= 0.72777 val_ap= 0.65520 time= 0.07679\n",
      "Epoch: 0064 train_loss= 0.73047 val_ap= 0.66278 time= 0.07879\n",
      "Epoch: 0065 train_loss= 0.72504 val_ap= 0.65573 time= 0.07280\n",
      "Epoch: 0066 train_loss= 0.72523 val_ap= 0.66255 time= 0.07779\n",
      "Epoch: 0067 train_loss= 0.72259 val_ap= 0.65668 time= 0.07779\n",
      "Epoch: 0068 train_loss= 0.72248 val_ap= 0.66358 time= 0.07879\n",
      "Epoch: 0069 train_loss= 0.71955 val_ap= 0.65970 time= 0.09076\n",
      "Epoch: 0070 train_loss= 0.71857 val_ap= 0.65712 time= 0.08378\n",
      "Epoch: 0071 train_loss= 0.71578 val_ap= 0.65655 time= 0.08178\n",
      "Epoch: 0072 train_loss= 0.71410 val_ap= 0.65891 time= 0.08577\n",
      "Epoch: 0073 train_loss= 0.71142 val_ap= 0.66355 time= 0.08976\n",
      "Epoch: 0074 train_loss= 0.71786 val_ap= 0.66795 time= 0.08577\n",
      "Epoch: 0075 train_loss= 0.70834 val_ap= 0.66176 time= 0.07979\n",
      "Epoch: 0076 train_loss= 0.70737 val_ap= 0.66161 time= 0.07281\n",
      "Epoch: 0077 train_loss= 0.70509 val_ap= 0.66547 time= 0.07280\n",
      "Epoch: 0078 train_loss= 0.70076 val_ap= 0.66333 time= 0.07580\n",
      "Epoch: 0079 train_loss= 0.70354 val_ap= 0.66269 time= 0.09076\n",
      "Epoch: 0080 train_loss= 0.69920 val_ap= 0.66993 time= 0.07580\n",
      "Epoch: 0081 train_loss= 0.69626 val_ap= 0.66261 time= 0.07380\n",
      "Epoch: 0082 train_loss= 0.69235 val_ap= 0.66394 time= 0.08277\n",
      "Epoch: 0083 train_loss= 0.69336 val_ap= 0.67419 time= 0.09176\n",
      "Epoch: 0084 train_loss= 0.68987 val_ap= 0.67398 time= 0.09674\n",
      "Epoch: 0085 train_loss= 0.68579 val_ap= 0.67291 time= 0.09475\n",
      "Epoch: 0086 train_loss= 0.68492 val_ap= 0.67579 time= 0.08378\n",
      "Epoch: 0087 train_loss= 0.68653 val_ap= 0.66473 time= 0.08278\n",
      "Epoch: 0088 train_loss= 0.67859 val_ap= 0.67381 time= 0.07679\n",
      "Epoch: 0089 train_loss= 0.67515 val_ap= 0.68085 time= 0.07779\n",
      "Epoch: 0090 train_loss= 0.67206 val_ap= 0.68203 time= 0.07380\n",
      "Epoch: 0091 train_loss= 0.66896 val_ap= 0.67971 time= 0.07779\n",
      "Epoch: 0092 train_loss= 0.66543 val_ap= 0.68100 time= 0.08777\n",
      "Epoch: 0093 train_loss= 0.66302 val_ap= 0.69126 time= 0.07679\n",
      "Epoch: 0094 train_loss= 0.65888 val_ap= 0.69059 time= 0.08876\n",
      "Epoch: 0095 train_loss= 0.66502 val_ap= 0.70213 time= 0.07979\n",
      "Epoch: 0096 train_loss= 0.65572 val_ap= 0.70363 time= 0.07580\n",
      "Epoch: 0097 train_loss= 0.65032 val_ap= 0.70543 time= 0.08377\n",
      "Epoch: 0098 train_loss= 0.64919 val_ap= 0.70236 time= 0.08178\n",
      "Epoch: 0099 train_loss= 0.65089 val_ap= 0.69997 time= 0.07580\n",
      "Epoch: 0100 train_loss= 0.65196 val_ap= 0.69414 time= 0.07479\n",
      "Epoch: 0101 train_loss= 0.63728 val_ap= 0.72236 time= 0.07281\n",
      "Epoch: 0102 train_loss= 0.63247 val_ap= 0.72611 time= 0.08576\n",
      "Epoch: 0103 train_loss= 0.63469 val_ap= 0.73334 time= 0.07580\n",
      "Epoch: 0104 train_loss= 0.62386 val_ap= 0.73344 time= 0.08378\n",
      "Epoch: 0105 train_loss= 0.62348 val_ap= 0.74125 time= 0.08078\n",
      "Epoch: 0106 train_loss= 0.61649 val_ap= 0.74339 time= 0.07879\n",
      "Epoch: 0107 train_loss= 0.61393 val_ap= 0.74848 time= 0.08178\n",
      "Epoch: 0108 train_loss= 0.60984 val_ap= 0.75331 time= 0.10272\n",
      "Epoch: 0109 train_loss= 0.60633 val_ap= 0.75944 time= 0.08178\n",
      "Epoch: 0110 train_loss= 0.60313 val_ap= 0.76474 time= 0.08178\n",
      "Epoch: 0111 train_loss= 0.59831 val_ap= 0.76778 time= 0.08278\n",
      "Epoch: 0112 train_loss= 0.59523 val_ap= 0.77329 time= 0.07580\n",
      "Epoch: 0113 train_loss= 0.59285 val_ap= 0.77833 time= 0.07480\n",
      "Epoch: 0114 train_loss= 0.58855 val_ap= 0.78219 time= 0.07981\n",
      "Epoch: 0115 train_loss= 0.58485 val_ap= 0.78507 time= 0.07477\n",
      "Epoch: 0116 train_loss= 0.58298 val_ap= 0.78847 time= 0.07580\n",
      "Epoch: 0117 train_loss= 0.57799 val_ap= 0.79419 time= 0.07979\n",
      "Epoch: 0118 train_loss= 0.57880 val_ap= 0.79738 time= 0.08278\n",
      "Epoch: 0119 train_loss= 0.57181 val_ap= 0.80063 time= 0.08876\n",
      "Epoch: 0120 train_loss= 0.57127 val_ap= 0.80591 time= 0.08378\n",
      "Epoch: 0121 train_loss= 0.56677 val_ap= 0.80646 time= 0.07480\n",
      "Epoch: 0122 train_loss= 0.56366 val_ap= 0.80745 time= 0.07380\n",
      "Epoch: 0123 train_loss= 0.57071 val_ap= 0.80266 time= 0.08178\n",
      "Epoch: 0124 train_loss= 0.56021 val_ap= 0.80894 time= 0.07779\n",
      "Epoch: 0125 train_loss= 0.55729 val_ap= 0.81554 time= 0.07281\n",
      "Epoch: 0126 train_loss= 0.55317 val_ap= 0.81681 time= 0.07380\n",
      "Epoch: 0127 train_loss= 0.55066 val_ap= 0.81860 time= 0.07380\n",
      "Epoch: 0128 train_loss= 0.54961 val_ap= 0.82217 time= 0.08477\n",
      "Epoch: 0129 train_loss= 0.54749 val_ap= 0.82356 time= 0.07579\n",
      "Epoch: 0130 train_loss= 0.54408 val_ap= 0.82389 time= 0.08178\n",
      "Epoch: 0131 train_loss= 0.54285 val_ap= 0.82293 time= 0.08178\n",
      "Epoch: 0132 train_loss= 0.54267 val_ap= 0.82257 time= 0.07380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0133 train_loss= 0.53687 val_ap= 0.82670 time= 0.07480\n",
      "Epoch: 0134 train_loss= 0.53547 val_ap= 0.83052 time= 0.08677\n",
      "Epoch: 0135 train_loss= 0.53554 val_ap= 0.83603 time= 0.08677\n",
      "Epoch: 0136 train_loss= 0.54499 val_ap= 0.83852 time= 0.09176\n",
      "Epoch: 0137 train_loss= 0.52824 val_ap= 0.83756 time= 0.07779\n",
      "Epoch: 0138 train_loss= 0.52807 val_ap= 0.83614 time= 0.07081\n",
      "Epoch: 0139 train_loss= 0.52605 val_ap= 0.83857 time= 0.07480\n",
      "Epoch: 0140 train_loss= 0.52382 val_ap= 0.84039 time= 0.07879\n",
      "Epoch: 0141 train_loss= 0.52255 val_ap= 0.84110 time= 0.08078\n",
      "Epoch: 0142 train_loss= 0.51974 val_ap= 0.84525 time= 0.07879\n",
      "Epoch: 0143 train_loss= 0.51805 val_ap= 0.84972 time= 0.08078\n",
      "Epoch: 0144 train_loss= 0.51686 val_ap= 0.85296 time= 0.08477\n",
      "Epoch: 0145 train_loss= 0.51417 val_ap= 0.85450 time= 0.08078\n",
      "Epoch: 0146 train_loss= 0.51595 val_ap= 0.85793 time= 0.07679\n",
      "Epoch: 0147 train_loss= 0.51131 val_ap= 0.85708 time= 0.07779\n",
      "Epoch: 0148 train_loss= 0.51061 val_ap= 0.85465 time= 0.09475\n",
      "Epoch: 0149 train_loss= 0.50921 val_ap= 0.85730 time= 0.07979\n",
      "Epoch: 0150 train_loss= 0.50678 val_ap= 0.86085 time= 0.07679\n",
      "Epoch: 0151 train_loss= 0.51067 val_ap= 0.85844 time= 0.07481\n",
      "Epoch: 0152 train_loss= 0.50438 val_ap= 0.86509 time= 0.08576\n",
      "Epoch: 0153 train_loss= 0.50343 val_ap= 0.86997 time= 0.07979\n",
      "Epoch: 0154 train_loss= 0.50261 val_ap= 0.87225 time= 0.07580\n",
      "Epoch: 0155 train_loss= 0.50075 val_ap= 0.87243 time= 0.08377\n",
      "Epoch: 0156 train_loss= 0.50154 val_ap= 0.87484 time= 0.07879\n",
      "Epoch: 0157 train_loss= 0.49774 val_ap= 0.87361 time= 0.07979\n",
      "Epoch: 0158 train_loss= 0.49697 val_ap= 0.87461 time= 0.08278\n",
      "Epoch: 0159 train_loss= 0.50019 val_ap= 0.87074 time= 0.08278\n",
      "Epoch: 0160 train_loss= 0.49865 val_ap= 0.87294 time= 0.08078\n",
      "Epoch: 0161 train_loss= 0.49432 val_ap= 0.88003 time= 0.07679\n",
      "Epoch: 0162 train_loss= 0.49332 val_ap= 0.88268 time= 0.07479\n",
      "Epoch: 0163 train_loss= 0.49326 val_ap= 0.88473 time= 0.07280\n",
      "Epoch: 0164 train_loss= 0.49535 val_ap= 0.88545 time= 0.07580\n",
      "Epoch: 0165 train_loss= 0.49064 val_ap= 0.88307 time= 0.08477\n",
      "Epoch: 0166 train_loss= 0.49202 val_ap= 0.88072 time= 0.07779\n",
      "Epoch: 0167 train_loss= 0.48860 val_ap= 0.88314 time= 0.08876\n",
      "Epoch: 0168 train_loss= 0.48766 val_ap= 0.88395 time= 0.08577\n",
      "Epoch: 0169 train_loss= 0.48730 val_ap= 0.88522 time= 0.08078\n",
      "Epoch: 0170 train_loss= 0.48667 val_ap= 0.88680 time= 0.07680\n",
      "Epoch: 0171 train_loss= 0.48575 val_ap= 0.88856 time= 0.09175\n",
      "Epoch: 0172 train_loss= 0.48454 val_ap= 0.88874 time= 0.07580\n",
      "Epoch: 0173 train_loss= 0.48619 val_ap= 0.89056 time= 0.07579\n",
      "Epoch: 0174 train_loss= 0.48331 val_ap= 0.88978 time= 0.07979\n",
      "Epoch: 0175 train_loss= 0.48283 val_ap= 0.89005 time= 0.07679\n",
      "Epoch: 0176 train_loss= 0.48222 val_ap= 0.89017 time= 0.07580\n",
      "Epoch: 0177 train_loss= 0.48257 val_ap= 0.88932 time= 0.07280\n",
      "Epoch: 0178 train_loss= 0.48073 val_ap= 0.89113 time= 0.09076\n",
      "Epoch: 0179 train_loss= 0.48039 val_ap= 0.89128 time= 0.08278\n",
      "Epoch: 0180 train_loss= 0.47962 val_ap= 0.89178 time= 0.07679\n",
      "Epoch: 0181 train_loss= 0.47951 val_ap= 0.89206 time= 0.08078\n",
      "Epoch: 0182 train_loss= 0.47842 val_ap= 0.89255 time= 0.08178\n",
      "Epoch: 0183 train_loss= 0.47800 val_ap= 0.89301 time= 0.07879\n",
      "Epoch: 0184 train_loss= 0.47736 val_ap= 0.89421 time= 0.08976\n",
      "Epoch: 0185 train_loss= 0.47700 val_ap= 0.89358 time= 0.07580\n",
      "Epoch: 0186 train_loss= 0.47691 val_ap= 0.89425 time= 0.07879\n",
      "Epoch: 0187 train_loss= 0.47608 val_ap= 0.89568 time= 0.07680\n",
      "Epoch: 0188 train_loss= 0.47503 val_ap= 0.89481 time= 0.07181\n",
      "Epoch: 0189 train_loss= 0.47496 val_ap= 0.89568 time= 0.08477\n",
      "Epoch: 0190 train_loss= 0.47486 val_ap= 0.89593 time= 0.07480\n",
      "Epoch: 0191 train_loss= 0.47370 val_ap= 0.89574 time= 0.07380\n",
      "Epoch: 0192 train_loss= 0.47335 val_ap= 0.89701 time= 0.07979\n",
      "Epoch: 0193 train_loss= 0.47335 val_ap= 0.89726 time= 0.07779\n",
      "Epoch: 0194 train_loss= 0.47303 val_ap= 0.89603 time= 0.08976\n",
      "Epoch: 0195 train_loss= 0.47219 val_ap= 0.89663 time= 0.08378\n",
      "Epoch: 0196 train_loss= 0.47190 val_ap= 0.89663 time= 0.08677\n",
      "Epoch: 0197 train_loss= 0.47110 val_ap= 0.89666 time= 0.07979\n",
      "Epoch: 0198 train_loss= 0.47103 val_ap= 0.89863 time= 0.08976\n",
      "Epoch: 0199 train_loss= 0.47035 val_ap= 0.89851 time= 0.08677\n",
      "Epoch: 0200 train_loss= 0.47030 val_ap= 0.89904 time= 0.09973\n",
      "Epoch: 0201 train_loss= 0.46980 val_ap= 0.89786 time= 0.09275\n",
      "Epoch: 0202 train_loss= 0.46853 val_ap= 0.89874 time= 0.09275\n",
      "Epoch: 0203 train_loss= 0.46812 val_ap= 0.89868 time= 0.08378\n",
      "Epoch: 0204 train_loss= 0.46771 val_ap= 0.89898 time= 0.08876\n",
      "Epoch: 0205 train_loss= 0.46760 val_ap= 0.89808 time= 0.08876\n",
      "Epoch: 0206 train_loss= 0.46694 val_ap= 0.89929 time= 0.07580\n",
      "Epoch: 0207 train_loss= 0.46666 val_ap= 0.89931 time= 0.08477\n",
      "Epoch: 0208 train_loss= 0.46644 val_ap= 0.90017 time= 0.07879\n",
      "Epoch: 0209 train_loss= 0.46636 val_ap= 0.90091 time= 0.09774\n",
      "Epoch: 0210 train_loss= 0.46528 val_ap= 0.89966 time= 0.07979\n",
      "Epoch: 0211 train_loss= 0.46486 val_ap= 0.90041 time= 0.07480\n",
      "Epoch: 0212 train_loss= 0.46457 val_ap= 0.90044 time= 0.08078\n",
      "Epoch: 0213 train_loss= 0.46409 val_ap= 0.90113 time= 0.08477\n",
      "Epoch: 0214 train_loss= 0.46359 val_ap= 0.90074 time= 0.09874\n",
      "Epoch: 0215 train_loss= 0.46316 val_ap= 0.90153 time= 0.07580\n",
      "Epoch: 0216 train_loss= 0.46297 val_ap= 0.90049 time= 0.07480\n",
      "Epoch: 0217 train_loss= 0.46283 val_ap= 0.90177 time= 0.08577\n",
      "Epoch: 0218 train_loss= 0.46219 val_ap= 0.90197 time= 0.08278\n",
      "Epoch: 0219 train_loss= 0.46241 val_ap= 0.90278 time= 0.08477\n",
      "Epoch: 0220 train_loss= 0.46144 val_ap= 0.90259 time= 0.08377\n",
      "Epoch: 0221 train_loss= 0.46157 val_ap= 0.90204 time= 0.08577\n",
      "Epoch: 0222 train_loss= 0.46069 val_ap= 0.90270 time= 0.07281\n",
      "Epoch: 0223 train_loss= 0.46043 val_ap= 0.90403 time= 0.08178\n",
      "Epoch: 0224 train_loss= 0.46073 val_ap= 0.90408 time= 0.07879\n",
      "Epoch: 0225 train_loss= 0.45949 val_ap= 0.90420 time= 0.07181\n",
      "Epoch: 0226 train_loss= 0.45920 val_ap= 0.90467 time= 0.07978\n",
      "Epoch: 0227 train_loss= 0.45973 val_ap= 0.90408 time= 0.07380\n",
      "Epoch: 0228 train_loss= 0.45860 val_ap= 0.90430 time= 0.07779\n",
      "Epoch: 0229 train_loss= 0.45816 val_ap= 0.90519 time= 0.08777\n",
      "Epoch: 0230 train_loss= 0.45783 val_ap= 0.90513 time= 0.07482\n",
      "Epoch: 0231 train_loss= 0.45786 val_ap= 0.90566 time= 0.08276\n",
      "Epoch: 0232 train_loss= 0.45738 val_ap= 0.90552 time= 0.08477\n",
      "Epoch: 0233 train_loss= 0.45685 val_ap= 0.90537 time= 0.07879\n",
      "Epoch: 0234 train_loss= 0.45690 val_ap= 0.90525 time= 0.07979\n",
      "Epoch: 0235 train_loss= 0.45666 val_ap= 0.90561 time= 0.08278\n",
      "Epoch: 0236 train_loss= 0.45591 val_ap= 0.90682 time= 0.07879\n",
      "Epoch: 0237 train_loss= 0.45576 val_ap= 0.90626 time= 0.07580\n",
      "Epoch: 0238 train_loss= 0.45536 val_ap= 0.90678 time= 0.08178\n",
      "Epoch: 0239 train_loss= 0.45503 val_ap= 0.90646 time= 0.07879\n",
      "Epoch: 0240 train_loss= 0.45544 val_ap= 0.90808 time= 0.07979\n",
      "Epoch: 0241 train_loss= 0.45465 val_ap= 0.90788 time= 0.08378\n",
      "Epoch: 0242 train_loss= 0.45425 val_ap= 0.90683 time= 0.07879\n",
      "Epoch: 0243 train_loss= 0.45383 val_ap= 0.90716 time= 0.08876\n",
      "Epoch: 0244 train_loss= 0.45351 val_ap= 0.90757 time= 0.09474\n",
      "Epoch: 0245 train_loss= 0.45335 val_ap= 0.90741 time= 0.08777\n",
      "Epoch: 0246 train_loss= 0.45287 val_ap= 0.90813 time= 0.07979\n",
      "Epoch: 0247 train_loss= 0.45294 val_ap= 0.90889 time= 0.08178\n",
      "Epoch: 0248 train_loss= 0.45258 val_ap= 0.90768 time= 0.08477\n",
      "Epoch: 0249 train_loss= 0.45207 val_ap= 0.90903 time= 0.08178\n",
      "Epoch: 0250 train_loss= 0.45168 val_ap= 0.90902 time= 0.08178\n",
      "Epoch: 0251 train_loss= 0.45151 val_ap= 0.90914 time= 0.08082\n",
      "Epoch: 0252 train_loss= 0.45123 val_ap= 0.90886 time= 0.08178\n",
      "Epoch: 0253 train_loss= 0.45102 val_ap= 0.90899 time= 0.08577\n",
      "Epoch: 0254 train_loss= 0.45059 val_ap= 0.90960 time= 0.07979\n",
      "Epoch: 0255 train_loss= 0.45036 val_ap= 0.90943 time= 0.07380\n",
      "Epoch: 0256 train_loss= 0.45012 val_ap= 0.91018 time= 0.08178\n",
      "Epoch: 0257 train_loss= 0.45000 val_ap= 0.91068 time= 0.08876\n",
      "Epoch: 0258 train_loss= 0.44972 val_ap= 0.91064 time= 0.08577\n",
      "Epoch: 0259 train_loss= 0.44938 val_ap= 0.91026 time= 0.07979\n",
      "Epoch: 0260 train_loss= 0.44895 val_ap= 0.91032 time= 0.08078\n",
      "Epoch: 0261 train_loss= 0.44878 val_ap= 0.91080 time= 0.08178\n",
      "Epoch: 0262 train_loss= 0.44865 val_ap= 0.91138 time= 0.08178\n",
      "Epoch: 0263 train_loss= 0.44841 val_ap= 0.91179 time= 0.08277\n",
      "Epoch: 0264 train_loss= 0.44788 val_ap= 0.91144 time= 0.09275\n",
      "Epoch: 0265 train_loss= 0.44773 val_ap= 0.91184 time= 0.08478\n",
      "Epoch: 0266 train_loss= 0.44745 val_ap= 0.91164 time= 0.09275\n",
      "Epoch: 0267 train_loss= 0.44727 val_ap= 0.91211 time= 0.07580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0268 train_loss= 0.44698 val_ap= 0.91225 time= 0.08477\n",
      "Epoch: 0269 train_loss= 0.44674 val_ap= 0.91263 time= 0.08677\n",
      "Epoch: 0270 train_loss= 0.44746 val_ap= 0.91326 time= 0.07879\n",
      "Epoch: 0271 train_loss= 0.44616 val_ap= 0.91292 time= 0.07779\n",
      "Epoch: 0272 train_loss= 0.44673 val_ap= 0.91289 time= 0.07779\n",
      "Epoch: 0273 train_loss= 0.44573 val_ap= 0.91345 time= 0.07779\n",
      "Epoch: 0274 train_loss= 0.44585 val_ap= 0.91329 time= 0.09274\n",
      "Epoch: 0275 train_loss= 0.44608 val_ap= 0.91426 time= 0.07580\n",
      "Epoch: 0276 train_loss= 0.44524 val_ap= 0.91394 time= 0.07281\n",
      "Epoch: 0277 train_loss= 0.44481 val_ap= 0.91379 time= 0.09275\n",
      "Epoch: 0278 train_loss= 0.44455 val_ap= 0.91448 time= 0.07679\n",
      "Epoch: 0279 train_loss= 0.44445 val_ap= 0.91473 time= 0.07380\n",
      "Epoch: 0280 train_loss= 0.44463 val_ap= 0.91454 time= 0.08078\n",
      "Epoch: 0281 train_loss= 0.44384 val_ap= 0.91499 time= 0.07480\n",
      "Epoch: 0282 train_loss= 0.44376 val_ap= 0.91513 time= 0.07380\n",
      "Epoch: 0283 train_loss= 0.44345 val_ap= 0.91512 time= 0.07979\n",
      "Epoch: 0284 train_loss= 0.44414 val_ap= 0.91610 time= 0.07281\n",
      "Epoch: 0285 train_loss= 0.44312 val_ap= 0.91616 time= 0.07281\n",
      "Epoch: 0286 train_loss= 0.44303 val_ap= 0.91612 time= 0.07976\n",
      "Epoch: 0287 train_loss= 0.44321 val_ap= 0.91625 time= 0.07779\n",
      "Epoch: 0288 train_loss= 0.44253 val_ap= 0.91635 time= 0.09475\n",
      "Epoch: 0289 train_loss= 0.44222 val_ap= 0.91646 time= 0.08876\n",
      "Epoch: 0290 train_loss= 0.44220 val_ap= 0.91677 time= 0.07480\n",
      "Epoch: 0291 train_loss= 0.44235 val_ap= 0.91724 time= 0.07580\n",
      "Epoch: 0292 train_loss= 0.44157 val_ap= 0.91728 time= 0.07779\n",
      "Epoch: 0293 train_loss= 0.44148 val_ap= 0.91768 time= 0.08577\n",
      "Epoch: 0294 train_loss= 0.44119 val_ap= 0.91735 time= 0.08477\n",
      "Epoch: 0295 train_loss= 0.44104 val_ap= 0.91800 time= 0.08178\n",
      "Epoch: 0296 train_loss= 0.44072 val_ap= 0.91786 time= 0.07879\n",
      "Epoch: 0297 train_loss= 0.44052 val_ap= 0.91785 time= 0.07281\n",
      "Epoch: 0298 train_loss= 0.44040 val_ap= 0.91853 time= 0.09574\n",
      "Epoch: 0299 train_loss= 0.44082 val_ap= 0.91899 time= 0.08278\n",
      "Epoch: 0300 train_loss= 0.44008 val_ap= 0.91852 time= 0.07879\n",
      "Epoch: 0301 train_loss= 0.43985 val_ap= 0.91909 time= 0.08078\n",
      "Epoch: 0302 train_loss= 0.43986 val_ap= 0.91892 time= 0.07480\n",
      "Epoch: 0303 train_loss= 0.44000 val_ap= 0.91848 time= 0.07480\n",
      "Epoch: 0304 train_loss= 0.43950 val_ap= 0.91901 time= 0.08577\n",
      "Epoch: 0305 train_loss= 0.43946 val_ap= 0.91979 time= 0.08577\n",
      "Epoch: 0306 train_loss= 0.43890 val_ap= 0.91955 time= 0.08677\n",
      "Epoch: 0307 train_loss= 0.43874 val_ap= 0.91996 time= 0.07580\n",
      "Epoch: 0308 train_loss= 0.43854 val_ap= 0.91979 time= 0.07879\n",
      "Epoch: 0309 train_loss= 0.43857 val_ap= 0.91947 time= 0.07480\n",
      "Epoch: 0310 train_loss= 0.43819 val_ap= 0.92002 time= 0.07480\n",
      "Epoch: 0311 train_loss= 0.43806 val_ap= 0.92029 time= 0.09076\n",
      "Epoch: 0312 train_loss= 0.43786 val_ap= 0.92035 time= 0.07679\n",
      "Epoch: 0313 train_loss= 0.43844 val_ap= 0.92075 time= 0.08377\n",
      "Epoch: 0314 train_loss= 0.43747 val_ap= 0.92056 time= 0.08278\n",
      "Epoch: 0315 train_loss= 0.43980 val_ap= 0.91922 time= 0.08278\n",
      "Epoch: 0316 train_loss= 0.43718 val_ap= 0.92044 time= 0.08178\n",
      "Epoch: 0317 train_loss= 0.43686 val_ap= 0.92091 time= 0.08378\n",
      "Epoch: 0318 train_loss= 0.43705 val_ap= 0.92112 time= 0.09076\n",
      "Epoch: 0319 train_loss= 0.43662 val_ap= 0.92124 time= 0.08078\n",
      "Epoch: 0320 train_loss= 0.43660 val_ap= 0.92142 time= 0.08477\n",
      "Epoch: 0321 train_loss= 0.43637 val_ap= 0.92124 time= 0.10173\n",
      "Epoch: 0322 train_loss= 0.43652 val_ap= 0.92075 time= 0.08078\n",
      "Epoch: 0323 train_loss= 0.43586 val_ap= 0.92138 time= 0.08477\n",
      "Epoch: 0324 train_loss= 0.43583 val_ap= 0.92144 time= 0.09574\n",
      "Epoch: 0325 train_loss= 0.43595 val_ap= 0.92217 time= 0.08178\n",
      "Epoch: 0326 train_loss= 0.43542 val_ap= 0.92174 time= 0.09574\n",
      "Epoch: 0327 train_loss= 0.43627 val_ap= 0.92156 time= 0.09076\n",
      "Epoch: 0328 train_loss= 0.43553 val_ap= 0.92186 time= 0.08178\n",
      "Epoch: 0329 train_loss= 0.43506 val_ap= 0.92264 time= 0.09574\n",
      "Epoch: 0330 train_loss= 0.43524 val_ap= 0.92295 time= 0.09475\n",
      "Epoch: 0331 train_loss= 0.43498 val_ap= 0.92282 time= 0.07380\n",
      "Epoch: 0332 train_loss= 0.43471 val_ap= 0.92244 time= 0.09176\n",
      "Epoch: 0333 train_loss= 0.43450 val_ap= 0.92251 time= 0.07779\n",
      "Epoch: 0334 train_loss= 0.43443 val_ap= 0.92310 time= 0.08777\n",
      "Epoch: 0335 train_loss= 0.43405 val_ap= 0.92267 time= 0.07480\n",
      "Epoch: 0336 train_loss= 0.43407 val_ap= 0.92289 time= 0.07580\n",
      "Epoch: 0337 train_loss= 0.43392 val_ap= 0.92336 time= 0.07380\n",
      "Epoch: 0338 train_loss= 0.43363 val_ap= 0.92340 time= 0.08278\n",
      "Epoch: 0339 train_loss= 0.43399 val_ap= 0.92377 time= 0.09076\n",
      "Epoch: 0340 train_loss= 0.43361 val_ap= 0.92320 time= 0.08178\n",
      "Epoch: 0341 train_loss= 0.43350 val_ap= 0.92334 time= 0.08576\n",
      "Epoch: 0342 train_loss= 0.43328 val_ap= 0.92398 time= 0.08378\n",
      "Epoch: 0343 train_loss= 0.43290 val_ap= 0.92380 time= 0.09175\n",
      "Epoch: 0344 train_loss= 0.43290 val_ap= 0.92384 time= 0.08976\n",
      "Epoch: 0345 train_loss= 0.43284 val_ap= 0.92433 time= 0.07280\n",
      "Epoch: 0346 train_loss= 0.43301 val_ap= 0.92451 time= 0.08477\n",
      "Epoch: 0347 train_loss= 0.43246 val_ap= 0.92428 time= 0.09574\n",
      "Epoch: 0348 train_loss= 0.43258 val_ap= 0.92385 time= 0.08677\n",
      "Epoch: 0349 train_loss= 0.43228 val_ap= 0.92403 time= 0.07380\n",
      "Epoch: 0350 train_loss= 0.43212 val_ap= 0.92426 time= 0.09175\n",
      "Epoch: 0351 train_loss= 0.43190 val_ap= 0.92503 time= 0.09076\n",
      "Epoch: 0352 train_loss= 0.43218 val_ap= 0.92527 time= 0.08278\n",
      "Epoch: 0353 train_loss= 0.43190 val_ap= 0.92546 time= 0.07580\n",
      "Epoch: 0354 train_loss= 0.43149 val_ap= 0.92445 time= 0.07480\n",
      "Epoch: 0355 train_loss= 0.43167 val_ap= 0.92441 time= 0.07281\n",
      "Epoch: 0356 train_loss= 0.43218 val_ap= 0.92436 time= 0.08178\n",
      "Epoch: 0357 train_loss= 0.43104 val_ap= 0.92531 time= 0.07979\n",
      "Epoch: 0358 train_loss= 0.43137 val_ap= 0.92591 time= 0.07979\n",
      "Epoch: 0359 train_loss= 0.43146 val_ap= 0.92573 time= 0.08677\n",
      "Epoch: 0360 train_loss= 0.43101 val_ap= 0.92586 time= 0.07580\n",
      "Epoch: 0361 train_loss= 0.43118 val_ap= 0.92499 time= 0.07281\n",
      "Epoch: 0362 train_loss= 0.43120 val_ap= 0.92510 time= 0.08976\n",
      "Epoch: 0363 train_loss= 0.43036 val_ap= 0.92573 time= 0.07480\n",
      "Epoch: 0364 train_loss= 0.43023 val_ap= 0.92598 time= 0.08378\n",
      "Epoch: 0365 train_loss= 0.43140 val_ap= 0.92646 time= 0.08577\n",
      "Epoch: 0366 train_loss= 0.43011 val_ap= 0.92616 time= 0.07779\n",
      "Epoch: 0367 train_loss= 0.43028 val_ap= 0.92566 time= 0.07281\n",
      "Epoch: 0368 train_loss= 0.43010 val_ap= 0.92583 time= 0.07679\n",
      "Epoch: 0369 train_loss= 0.43002 val_ap= 0.92568 time= 0.08078\n",
      "Epoch: 0370 train_loss= 0.42953 val_ap= 0.92643 time= 0.08078\n",
      "Epoch: 0371 train_loss= 0.42979 val_ap= 0.92679 time= 0.08777\n",
      "Epoch: 0372 train_loss= 0.42976 val_ap= 0.92715 time= 0.08577\n",
      "Epoch: 0373 train_loss= 0.42920 val_ap= 0.92679 time= 0.08577\n",
      "Epoch: 0374 train_loss= 0.42921 val_ap= 0.92642 time= 0.08677\n",
      "Epoch: 0375 train_loss= 0.43006 val_ap= 0.92653 time= 0.08078\n",
      "Epoch: 0376 train_loss= 0.42927 val_ap= 0.92660 time= 0.08078\n",
      "Epoch: 0377 train_loss= 0.42924 val_ap= 0.92779 time= 0.09076\n",
      "Epoch: 0378 train_loss= 0.42878 val_ap= 0.92703 time= 0.09674\n",
      "Epoch: 0379 train_loss= 0.42867 val_ap= 0.92698 time= 0.08178\n",
      "Epoch: 0380 train_loss= 0.42840 val_ap= 0.92685 time= 0.08278\n",
      "Epoch: 0381 train_loss= 0.42843 val_ap= 0.92714 time= 0.07779\n",
      "Epoch: 0382 train_loss= 0.42839 val_ap= 0.92741 time= 0.08278\n",
      "Epoch: 0383 train_loss= 0.42806 val_ap= 0.92742 time= 0.07779\n",
      "Epoch: 0384 train_loss= 0.42802 val_ap= 0.92798 time= 0.08477\n",
      "Epoch: 0385 train_loss= 0.42798 val_ap= 0.92807 time= 0.08378\n",
      "Epoch: 0386 train_loss= 0.42773 val_ap= 0.92751 time= 0.08378\n",
      "Epoch: 0387 train_loss= 0.42769 val_ap= 0.92732 time= 0.07580\n",
      "Epoch: 0388 train_loss= 0.42753 val_ap= 0.92852 time= 0.08378\n",
      "Epoch: 0389 train_loss= 0.42739 val_ap= 0.92795 time= 0.07779\n",
      "Epoch: 0390 train_loss= 0.42726 val_ap= 0.92792 time= 0.08078\n",
      "Epoch: 0391 train_loss= 0.42714 val_ap= 0.92811 time= 0.07779\n",
      "Epoch: 0392 train_loss= 0.42724 val_ap= 0.92827 time= 0.08776\n",
      "Epoch: 0393 train_loss= 0.42706 val_ap= 0.92824 time= 0.09574\n",
      "Epoch: 0394 train_loss= 0.42693 val_ap= 0.92882 time= 0.07979\n",
      "Epoch: 0395 train_loss= 0.42675 val_ap= 0.92887 time= 0.07480\n",
      "Epoch: 0396 train_loss= 0.42666 val_ap= 0.92896 time= 0.07480\n",
      "Epoch: 0397 train_loss= 0.42656 val_ap= 0.92856 time= 0.07979\n",
      "Epoch: 0398 train_loss= 0.42645 val_ap= 0.92886 time= 0.07979\n",
      "Epoch: 0399 train_loss= 0.42650 val_ap= 0.92896 time= 0.07779\n",
      "Epoch: 0400 train_loss= 0.42632 val_ap= 0.92899 time= 0.08477\n",
      "Epoch: 0401 train_loss= 0.42610 val_ap= 0.92913 time= 0.08178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0402 train_loss= 0.42615 val_ap= 0.92923 time= 0.09873\n",
      "Epoch: 0403 train_loss= 0.42591 val_ap= 0.92928 time= 0.07380\n",
      "Epoch: 0404 train_loss= 0.42586 val_ap= 0.92933 time= 0.07679\n",
      "Epoch: 0405 train_loss= 0.42572 val_ap= 0.92933 time= 0.07480\n",
      "Epoch: 0406 train_loss= 0.42566 val_ap= 0.92945 time= 0.07181\n",
      "Epoch: 0407 train_loss= 0.42556 val_ap= 0.92961 time= 0.08078\n",
      "Epoch: 0408 train_loss= 0.42547 val_ap= 0.92992 time= 0.08078\n",
      "Epoch: 0409 train_loss= 0.42560 val_ap= 0.93020 time= 0.09275\n",
      "Epoch: 0410 train_loss= 0.42523 val_ap= 0.92990 time= 0.08078\n",
      "Epoch: 0411 train_loss= 0.42521 val_ap= 0.92968 time= 0.07480\n",
      "Epoch: 0412 train_loss= 0.42562 val_ap= 0.92949 time= 0.07679\n",
      "Epoch: 0413 train_loss= 0.42543 val_ap= 0.92931 time= 0.08278\n",
      "Epoch: 0414 train_loss= 0.42510 val_ap= 0.93025 time= 0.08178\n",
      "Epoch: 0415 train_loss= 0.42485 val_ap= 0.93034 time= 0.07380\n",
      "Epoch: 0416 train_loss= 0.42521 val_ap= 0.93074 time= 0.07181\n",
      "Epoch: 0417 train_loss= 0.42469 val_ap= 0.93027 time= 0.07580\n",
      "Epoch: 0418 train_loss= 0.42476 val_ap= 0.93008 time= 0.07879\n",
      "Epoch: 0419 train_loss= 0.42454 val_ap= 0.93019 time= 0.08577\n",
      "Epoch: 0420 train_loss= 0.42431 val_ap= 0.93059 time= 0.08378\n",
      "Epoch: 0421 train_loss= 0.42420 val_ap= 0.93066 time= 0.08178\n",
      "Epoch: 0422 train_loss= 0.42433 val_ap= 0.93093 time= 0.07879\n",
      "Epoch: 0423 train_loss= 0.42403 val_ap= 0.93087 time= 0.07779\n",
      "Epoch: 0424 train_loss= 0.42394 val_ap= 0.93077 time= 0.07580\n",
      "Epoch: 0425 train_loss= 0.42405 val_ap= 0.93061 time= 0.08378\n",
      "Epoch: 0426 train_loss= 0.42377 val_ap= 0.93072 time= 0.08178\n",
      "Epoch: 0427 train_loss= 0.42363 val_ap= 0.93108 time= 0.08976\n",
      "Epoch: 0428 train_loss= 0.42355 val_ap= 0.93117 time= 0.07879\n",
      "Epoch: 0429 train_loss= 0.42381 val_ap= 0.93171 time= 0.08078\n",
      "Epoch: 0430 train_loss= 0.42351 val_ap= 0.93171 time= 0.07779\n",
      "Epoch: 0431 train_loss= 0.42348 val_ap= 0.93125 time= 0.07580\n",
      "Epoch: 0432 train_loss= 0.42413 val_ap= 0.93064 time= 0.08777\n",
      "Epoch: 0433 train_loss= 0.42318 val_ap= 0.93140 time= 0.07480\n",
      "Epoch: 0434 train_loss= 0.42325 val_ap= 0.93190 time= 0.07779\n",
      "Epoch: 0435 train_loss= 0.42352 val_ap= 0.93206 time= 0.08378\n",
      "Epoch: 0436 train_loss= 0.42282 val_ap= 0.93159 time= 0.07879\n",
      "Epoch: 0437 train_loss= 0.42283 val_ap= 0.93162 time= 0.07480\n",
      "Epoch: 0438 train_loss= 0.42306 val_ap= 0.93156 time= 0.08178\n",
      "Epoch: 0439 train_loss= 0.42268 val_ap= 0.93170 time= 0.08876\n",
      "Epoch: 0440 train_loss= 0.42255 val_ap= 0.93191 time= 0.07480\n",
      "Epoch: 0441 train_loss= 0.42253 val_ap= 0.93242 time= 0.07281\n",
      "Epoch: 0442 train_loss= 0.42284 val_ap= 0.93227 time= 0.07580\n",
      "Epoch: 0443 train_loss= 0.42226 val_ap= 0.93214 time= 0.08477\n",
      "Epoch: 0444 train_loss= 0.42248 val_ap= 0.93179 time= 0.07979\n",
      "Epoch: 0445 train_loss= 0.42262 val_ap= 0.93214 time= 0.07779\n",
      "Epoch: 0446 train_loss= 0.42223 val_ap= 0.93215 time= 0.07979\n",
      "Epoch: 0447 train_loss= 0.42259 val_ap= 0.93293 time= 0.08078\n",
      "Epoch: 0448 train_loss= 0.42210 val_ap= 0.93221 time= 0.07580\n",
      "Epoch: 0449 train_loss= 0.42191 val_ap= 0.93225 time= 0.07979\n",
      "Epoch: 0450 train_loss= 0.42179 val_ap= 0.93260 time= 0.07779\n",
      "Epoch: 0451 train_loss= 0.42173 val_ap= 0.93233 time= 0.07879\n",
      "Epoch: 0452 train_loss= 0.42158 val_ap= 0.93273 time= 0.08178\n",
      "Epoch: 0453 train_loss= 0.42148 val_ap= 0.93272 time= 0.07380\n",
      "Epoch: 0454 train_loss= 0.42164 val_ap= 0.93305 time= 0.07181\n",
      "Epoch: 0455 train_loss= 0.42145 val_ap= 0.93323 time= 0.07580\n",
      "Epoch: 0456 train_loss= 0.42123 val_ap= 0.93275 time= 0.07480\n",
      "Epoch: 0457 train_loss= 0.42157 val_ap= 0.93306 time= 0.07580\n",
      "Epoch: 0458 train_loss= 0.42111 val_ap= 0.93305 time= 0.08178\n",
      "Epoch: 0459 train_loss= 0.42107 val_ap= 0.93285 time= 0.07480\n",
      "Epoch: 0460 train_loss= 0.42086 val_ap= 0.93305 time= 0.07580\n",
      "Epoch: 0461 train_loss= 0.42106 val_ap= 0.93273 time= 0.09175\n",
      "Epoch: 0462 train_loss= 0.42084 val_ap= 0.93366 time= 0.09475\n",
      "Epoch: 0463 train_loss= 0.42083 val_ap= 0.93368 time= 0.08178\n",
      "Epoch: 0464 train_loss= 0.42060 val_ap= 0.93327 time= 0.08078\n",
      "Epoch: 0465 train_loss= 0.42061 val_ap= 0.93320 time= 0.07979\n",
      "Epoch: 0466 train_loss= 0.42066 val_ap= 0.93353 time= 0.07480\n",
      "Epoch: 0467 train_loss= 0.42042 val_ap= 0.93353 time= 0.09574\n",
      "Epoch: 0468 train_loss= 0.42045 val_ap= 0.93335 time= 0.10472\n",
      "Epoch: 0469 train_loss= 0.42053 val_ap= 0.93348 time= 0.08477\n",
      "Epoch: 0470 train_loss= 0.42022 val_ap= 0.93370 time= 0.07979\n",
      "Epoch: 0471 train_loss= 0.42029 val_ap= 0.93368 time= 0.07979\n",
      "Epoch: 0472 train_loss= 0.42003 val_ap= 0.93380 time= 0.09574\n",
      "Epoch: 0473 train_loss= 0.42040 val_ap= 0.93444 time= 0.09076\n",
      "Epoch: 0474 train_loss= 0.42010 val_ap= 0.93417 time= 0.07580\n",
      "Epoch: 0475 train_loss= 0.41991 val_ap= 0.93385 time= 0.07679\n",
      "Epoch: 0476 train_loss= 0.41987 val_ap= 0.93384 time= 0.08876\n",
      "Epoch: 0477 train_loss= 0.41971 val_ap= 0.93418 time= 0.08577\n",
      "Epoch: 0478 train_loss= 0.41980 val_ap= 0.93417 time= 0.08577\n",
      "Epoch: 0479 train_loss= 0.41989 val_ap= 0.93443 time= 0.07380\n",
      "Epoch: 0480 train_loss= 0.41949 val_ap= 0.93400 time= 0.07680\n",
      "Epoch: 0481 train_loss= 0.41943 val_ap= 0.93414 time= 0.07480\n",
      "Epoch: 0482 train_loss= 0.41940 val_ap= 0.93474 time= 0.08777\n",
      "Epoch: 0483 train_loss= 0.41928 val_ap= 0.93449 time= 0.08976\n",
      "Epoch: 0484 train_loss= 0.41917 val_ap= 0.93494 time= 0.08976\n",
      "Epoch: 0485 train_loss= 0.41931 val_ap= 0.93522 time= 0.07580\n",
      "Epoch: 0486 train_loss= 0.41895 val_ap= 0.93509 time= 0.07679\n",
      "Epoch: 0487 train_loss= 0.41893 val_ap= 0.93502 time= 0.09175\n",
      "Epoch: 0488 train_loss= 0.41890 val_ap= 0.93472 time= 0.08677\n",
      "Epoch: 0489 train_loss= 0.41911 val_ap= 0.93509 time= 0.08378\n",
      "Epoch: 0490 train_loss= 0.41875 val_ap= 0.93507 time= 0.07480\n",
      "Epoch: 0491 train_loss= 0.41906 val_ap= 0.93495 time= 0.07480\n",
      "Epoch: 0492 train_loss= 0.41865 val_ap= 0.93559 time= 0.07380\n",
      "Epoch: 0493 train_loss= 0.41852 val_ap= 0.93565 time= 0.09575\n",
      "Epoch: 0494 train_loss= 0.41884 val_ap= 0.93603 time= 0.08777\n",
      "Epoch: 0495 train_loss= 0.41832 val_ap= 0.93565 time= 0.08178\n",
      "Epoch: 0496 train_loss= 0.41828 val_ap= 0.93555 time= 0.07779\n",
      "Epoch: 0497 train_loss= 0.41828 val_ap= 0.93551 time= 0.07779\n",
      "Epoch: 0498 train_loss= 0.41832 val_ap= 0.93578 time= 0.07680\n",
      "Epoch: 0499 train_loss= 0.41804 val_ap= 0.93602 time= 0.09674\n",
      "Epoch: 0500 train_loss= 0.41800 val_ap= 0.93639 time= 0.09175\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9330354410234437\n",
      "Test AP score: 0.9401536566043232\n"
     ]
    }
   ],
   "source": [
    "# if once = False, run the model for 10 times with different random seeds.\n",
    "# if plot = True, then plot the learning curve every 10 epochs.\n",
    "once = True\n",
    "plot = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        gae_for(args, plot)\n",
    "    else:\n",
    "        test_roc = []\n",
    "        test_ap = []\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            roc_score, ap_score = gae_for(args,plot)\n",
    "            test_roc.append(roc_score)\n",
    "            test_ap.append(ap_score)\n",
    "        print(test_roc)\n",
    "        print('mean test AUC is',np.mean(test_roc),' std ', np.std(test_roc))\n",
    "        print(test_ap)\n",
    "        print('mean test AP is ',np.mean(test_ap), ' std ', np.std(test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
