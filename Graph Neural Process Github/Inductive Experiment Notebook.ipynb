{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from model import GNP_Encoder, GNP_Decoder, InnerProductDecoder\n",
    "from optimizer import loss_function3\n",
    "from utils import load_data, preprocess_graph, get_roc_score, train_val_test_split, ct_split_nodes\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e9e410db10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500, help='Number of epochs to train.')\n",
    "parser.add_argument('--hiddenEnc', type=int, default=32, help='Number of units in hidden layer of Encoder.')\n",
    "parser.add_argument('--z_dim', type=int, default=32, help='Dimension of latent code Z.')\n",
    "parser.add_argument('--hiddenDec', type=int, default=64, help='Number of units in hidden layer of Decoder.')\n",
    "parser.add_argument('--outDimDec', type=int, default=32, help='Output Dimension of the Decoder.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset_str', type=str, default='cora', help='type of dataset.')\n",
    "parser.add_argument('--n_z_samples', type=int, default=10, help='Number of Z samples')\n",
    "\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, std, n):\n",
    "    \"\"\"Reparameterisation trick.\"\"\"\n",
    "    eps = torch.autograd.Variable(std.data.new(n,args.z_dim).normal_())\n",
    "    return mu + std * eps \n",
    "\n",
    "def KLD_gaussian(mu_q, std_q, mu_p, std_p):\n",
    "    \"\"\"Analytical KLD between 2 Gaussians.\"\"\"\n",
    "    qs2 = std_q**2 + 1e-16\n",
    "    ps2 = std_p**2 + 1e-16\n",
    "    \n",
    "    return (qs2/ps2 + ((mu_q-mu_p)**2)/ps2 + torch.log(ps2/qs2) - 1.0).sum()*0.5\n",
    "    \n",
    "def GNP(args,if_plot):\n",
    "    adj, features = load_data(args.dataset_str)\n",
    "    n_nodes, feat_dim, = features.shape\n",
    "    features = features.to(device)\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))   \n",
    "    \n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # You can change the proportion of nodes for training and testing for different few-shot inductive tasks.\n",
    "    # Change testing nodes proportion in train_val_test_split from utils.py \n",
    "    # This notebook use an example of training with 30% nodes and around 10% links, \n",
    "    # to predict the rest 90% links in testing.\n",
    "    features_train, adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false \\\n",
    "    = train_val_test_split(features,adj)\n",
    "    adj = adj_train\n",
    "    adj_deep_copy = adj\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj + sp.eye(adj.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    encoder = GNP_Encoder(feat_dim, args.hiddenEnc, args.z_dim, args.dropout)\n",
    "    decoder = GNP_Decoder(feat_dim + args.z_dim, args.hiddenDec, args.outDimDec, args.dropout)\n",
    "    innerDecoder = InnerProductDecoder(args.dropout, act=lambda x: x)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    innerDecoder.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(decoder.parameters())+list(encoder.parameters()), args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_ap = []\n",
    "    for epoch in range(args.epochs):\n",
    "        \n",
    "        \n",
    "        t = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        innerDecoder.train()\n",
    "        \n",
    "        \n",
    "        np.random.seed(epoch)\n",
    "        features_context, adj_context  = ct_split_nodes(features_train, adj_deep_copy )\n",
    "        adj_context_norm = preprocess_graph(adj_context)\n",
    "        adj_context_norm = adj_context_norm.to(device) \n",
    "        \n",
    "        c_z_mu, c_z_logvar = encoder(features_context, adj_context_norm)\n",
    "        ct_z_mu, ct_z_logvar = encoder(features_train, adj_norm)\n",
    "        \n",
    "        #Sample a batch of zs using reparam trick.\n",
    "        zs = sample_z(ct_z_mu, torch.exp(ct_z_logvar), args.n_z_samples)\n",
    "        zs = zs.to(device)\n",
    "        \n",
    "        # Get the predictive distribution of y*\n",
    "        mu, std = decoder(features_train, zs)\n",
    "\n",
    "        emb = torch.mean(mu, dim = 1)\n",
    "        pred_adj = innerDecoder(emb)\n",
    "        \n",
    "        #Compute loss and backprop\n",
    "        loss = loss_function3(preds=pred_adj, labels=adj_label, norm=norm, pos_weight=torch.tensor(pos_weight)) + KLD_gaussian(ct_z_mu, torch.exp(ct_z_logvar), c_z_mu, torch.exp(c_z_logvar))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()        \n",
    "        scheduler.step()\n",
    "        \n",
    "        decoder.eval()\n",
    "        mu, std = decoder(features, zs)\n",
    "        emb = torch.mean(mu, dim = 1)\n",
    "        hidden_emb = emb.cpu().data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "        \n",
    "\n",
    "        train_loss.append(cur_loss)\n",
    "        val_ap.append(ap_curr)\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        \n",
    "        if (if_plot == True) and (epoch%10 == 0):\n",
    "            fig = plt.figure(figsize=(30,10))\n",
    "            ax1 = fig.add_subplot(1,2,1)\n",
    "            ax2 = fig.add_subplot(1,2,2)\n",
    "            \n",
    "            ax1.plot(train_loss, label='Training loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend(frameon=False)\n",
    "    \n",
    "            ax2.plot(val_ap, label='Validation Average Precision Score',color='Red')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('AP')\n",
    "            ax2.legend(frameon=False)\n",
    "            \n",
    "            plt.show()    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    return roc_score, ap_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using cora dataset\n",
      "number of training edges: 415\n",
      "number of total edges: 5278\n",
      "training edges/total edges: 0.0786282682834407\n",
      "Epoch: 0001 train_loss= 6.38263 val_ap= 0.57465 time= 0.55137\n",
      "Epoch: 0002 train_loss= 1.88367 val_ap= 0.59217 time= 0.10671\n",
      "Epoch: 0003 train_loss= 0.95612 val_ap= 0.55384 time= 0.13763\n",
      "Epoch: 0004 train_loss= 1.05494 val_ap= 0.56176 time= 0.13863\n",
      "Epoch: 0005 train_loss= 1.33812 val_ap= 0.53963 time= 0.10574\n",
      "Epoch: 0006 train_loss= 1.30587 val_ap= 0.52352 time= 0.11669\n",
      "Epoch: 0007 train_loss= 1.10163 val_ap= 0.49325 time= 0.12766\n",
      "Epoch: 0008 train_loss= 1.04751 val_ap= 0.49824 time= 0.10173\n",
      "Epoch: 0009 train_loss= 1.04071 val_ap= 0.49741 time= 0.11968\n",
      "Epoch: 0010 train_loss= 0.95398 val_ap= 0.49138 time= 0.11569\n",
      "Epoch: 0011 train_loss= 0.94534 val_ap= 0.50172 time= 0.11170\n",
      "Epoch: 0012 train_loss= 0.93209 val_ap= 0.52793 time= 0.13564\n",
      "Epoch: 0013 train_loss= 0.93427 val_ap= 0.53982 time= 0.12566\n",
      "Epoch: 0014 train_loss= 0.93473 val_ap= 0.59143 time= 0.12965\n",
      "Epoch: 0015 train_loss= 0.93924 val_ap= 0.60601 time= 0.11968\n",
      "Epoch: 0016 train_loss= 0.94414 val_ap= 0.59963 time= 0.13463\n",
      "Epoch: 0017 train_loss= 0.92376 val_ap= 0.59961 time= 0.11569\n",
      "Epoch: 0018 train_loss= 0.92767 val_ap= 0.62768 time= 0.11868\n",
      "Epoch: 0019 train_loss= 0.93160 val_ap= 0.64254 time= 0.13065\n",
      "Epoch: 0020 train_loss= 0.91507 val_ap= 0.67652 time= 0.11370\n",
      "Epoch: 0021 train_loss= 0.91199 val_ap= 0.68306 time= 0.14661\n",
      "Epoch: 0022 train_loss= 0.90743 val_ap= 0.63162 time= 0.12566\n",
      "Epoch: 0023 train_loss= 0.90578 val_ap= 0.61361 time= 0.11968\n",
      "Epoch: 0024 train_loss= 0.90007 val_ap= 0.67728 time= 0.10970\n",
      "Epoch: 0025 train_loss= 0.89945 val_ap= 0.66417 time= 0.13265\n",
      "Epoch: 0026 train_loss= 0.89568 val_ap= 0.66446 time= 0.10173\n",
      "Epoch: 0027 train_loss= 0.88661 val_ap= 0.66489 time= 0.13065\n",
      "Epoch: 0028 train_loss= 0.88229 val_ap= 0.61149 time= 0.10871\n",
      "Epoch: 0029 train_loss= 0.88006 val_ap= 0.59228 time= 0.10273\n",
      "Epoch: 0030 train_loss= 0.88278 val_ap= 0.60300 time= 0.10173\n",
      "Epoch: 0031 train_loss= 0.86699 val_ap= 0.62299 time= 0.11070\n",
      "Epoch: 0032 train_loss= 0.85839 val_ap= 0.59866 time= 0.10472\n",
      "Epoch: 0033 train_loss= 0.86686 val_ap= 0.66897 time= 0.10572\n",
      "Epoch: 0034 train_loss= 0.86468 val_ap= 0.63073 time= 0.12467\n",
      "Epoch: 0035 train_loss= 0.84878 val_ap= 0.62139 time= 0.11170\n",
      "Epoch: 0036 train_loss= 0.84367 val_ap= 0.61543 time= 0.12367\n",
      "Epoch: 0037 train_loss= 0.83959 val_ap= 0.59891 time= 0.10971\n",
      "Epoch: 0038 train_loss= 0.83544 val_ap= 0.60669 time= 0.11469\n",
      "Epoch: 0039 train_loss= 0.83009 val_ap= 0.60598 time= 0.10971\n",
      "Epoch: 0040 train_loss= 0.82791 val_ap= 0.61275 time= 0.12267\n",
      "Epoch: 0041 train_loss= 0.82646 val_ap= 0.60044 time= 0.11868\n",
      "Epoch: 0042 train_loss= 0.81855 val_ap= 0.59093 time= 0.11170\n",
      "Epoch: 0043 train_loss= 0.80060 val_ap= 0.60615 time= 0.09973\n",
      "Epoch: 0044 train_loss= 0.79068 val_ap= 0.59320 time= 0.10672\n",
      "Epoch: 0045 train_loss= 0.78544 val_ap= 0.59320 time= 0.12265\n",
      "Epoch: 0046 train_loss= 0.78425 val_ap= 0.57594 time= 0.13364\n",
      "Epoch: 0047 train_loss= 0.76931 val_ap= 0.59810 time= 0.11269\n",
      "Epoch: 0048 train_loss= 0.76570 val_ap= 0.58426 time= 0.12866\n",
      "Epoch: 0049 train_loss= 0.76081 val_ap= 0.60218 time= 0.11469\n",
      "Epoch: 0050 train_loss= 0.75943 val_ap= 0.56628 time= 0.12466\n",
      "Epoch: 0051 train_loss= 0.74388 val_ap= 0.56262 time= 0.11469\n",
      "Epoch: 0052 train_loss= 0.74581 val_ap= 0.55127 time= 0.12766\n",
      "Epoch: 0053 train_loss= 0.72980 val_ap= 0.56774 time= 0.11469\n",
      "Epoch: 0054 train_loss= 0.72430 val_ap= 0.55847 time= 0.11170\n",
      "Epoch: 0055 train_loss= 0.72233 val_ap= 0.55373 time= 0.12068\n",
      "Epoch: 0056 train_loss= 0.70937 val_ap= 0.56199 time= 0.14461\n",
      "Epoch: 0057 train_loss= 0.69871 val_ap= 0.56265 time= 0.11669\n",
      "Epoch: 0058 train_loss= 0.69234 val_ap= 0.57113 time= 0.12467\n",
      "Epoch: 0059 train_loss= 0.68288 val_ap= 0.55359 time= 0.13463\n",
      "Epoch: 0060 train_loss= 0.67586 val_ap= 0.55739 time= 0.11369\n",
      "Epoch: 0061 train_loss= 0.67202 val_ap= 0.57108 time= 0.11270\n",
      "Epoch: 0062 train_loss= 0.67234 val_ap= 0.54115 time= 0.11170\n",
      "Epoch: 0063 train_loss= 0.65027 val_ap= 0.54795 time= 0.12367\n",
      "Epoch: 0064 train_loss= 0.64993 val_ap= 0.54273 time= 0.13065\n",
      "Epoch: 0065 train_loss= 0.63623 val_ap= 0.55090 time= 0.10971\n",
      "Epoch: 0066 train_loss= 0.63366 val_ap= 0.55703 time= 0.10372\n",
      "Epoch: 0067 train_loss= 0.62284 val_ap= 0.56441 time= 0.12267\n",
      "Epoch: 0068 train_loss= 0.62578 val_ap= 0.56257 time= 0.10472\n",
      "Epoch: 0069 train_loss= 0.61094 val_ap= 0.58142 time= 0.12168\n",
      "Epoch: 0070 train_loss= 0.61100 val_ap= 0.58195 time= 0.13065\n",
      "Epoch: 0071 train_loss= 0.60203 val_ap= 0.57480 time= 0.11369\n",
      "Epoch: 0072 train_loss= 0.59026 val_ap= 0.57173 time= 0.09973\n",
      "Epoch: 0073 train_loss= 0.58572 val_ap= 0.56491 time= 0.10572\n",
      "Epoch: 0074 train_loss= 0.60380 val_ap= 0.55623 time= 0.11968\n",
      "Epoch: 0075 train_loss= 0.57505 val_ap= 0.56171 time= 0.10572\n",
      "Epoch: 0076 train_loss= 0.57319 val_ap= 0.58612 time= 0.11569\n",
      "Epoch: 0077 train_loss= 0.56440 val_ap= 0.58426 time= 0.12865\n",
      "Epoch: 0078 train_loss= 0.56622 val_ap= 0.59229 time= 0.11270\n",
      "Epoch: 0079 train_loss= 0.56730 val_ap= 0.59864 time= 0.11370\n",
      "Epoch: 0080 train_loss= 0.55695 val_ap= 0.57394 time= 0.10971\n",
      "Epoch: 0081 train_loss= 0.55159 val_ap= 0.59149 time= 0.13663\n",
      "Epoch: 0082 train_loss= 0.54678 val_ap= 0.58345 time= 0.11370\n",
      "Epoch: 0083 train_loss= 0.55544 val_ap= 0.59296 time= 0.11070\n",
      "Epoch: 0084 train_loss= 0.54944 val_ap= 0.59453 time= 0.10472\n",
      "Epoch: 0085 train_loss= 0.53881 val_ap= 0.61216 time= 0.12068\n",
      "Epoch: 0086 train_loss= 0.53640 val_ap= 0.61483 time= 0.11569\n",
      "Epoch: 0087 train_loss= 0.56252 val_ap= 0.62390 time= 0.09874\n",
      "Epoch: 0088 train_loss= 0.53621 val_ap= 0.61483 time= 0.09973\n",
      "Epoch: 0089 train_loss= 0.53020 val_ap= 0.62408 time= 0.10771\n",
      "Epoch: 0090 train_loss= 0.52807 val_ap= 0.62904 time= 0.12267\n",
      "Epoch: 0091 train_loss= 0.52467 val_ap= 0.62816 time= 0.11270\n",
      "Epoch: 0092 train_loss= 0.52097 val_ap= 0.63480 time= 0.10372\n",
      "Epoch: 0093 train_loss= 0.52056 val_ap= 0.63143 time= 0.09973\n",
      "Epoch: 0094 train_loss= 0.51796 val_ap= 0.65641 time= 0.11170\n",
      "Epoch: 0095 train_loss= 0.52253 val_ap= 0.65121 time= 0.12267\n",
      "Epoch: 0096 train_loss= 0.51358 val_ap= 0.66513 time= 0.10572\n",
      "Epoch: 0097 train_loss= 0.51976 val_ap= 0.65721 time= 0.12467\n",
      "Epoch: 0098 train_loss= 0.51323 val_ap= 0.66701 time= 0.11170\n",
      "Epoch: 0099 train_loss= 0.51226 val_ap= 0.67932 time= 0.10671\n",
      "Epoch: 0100 train_loss= 0.52771 val_ap= 0.68604 time= 0.10173\n",
      "Epoch: 0101 train_loss= 0.50795 val_ap= 0.64363 time= 0.10671\n",
      "Epoch: 0102 train_loss= 0.50556 val_ap= 0.64944 time= 0.11370\n",
      "Epoch: 0103 train_loss= 0.51624 val_ap= 0.65373 time= 0.12467\n",
      "Epoch: 0104 train_loss= 0.49864 val_ap= 0.67170 time= 0.10372\n",
      "Epoch: 0105 train_loss= 0.49915 val_ap= 0.67537 time= 0.09674\n",
      "Epoch: 0106 train_loss= 0.49679 val_ap= 0.69912 time= 0.10572\n",
      "Epoch: 0107 train_loss= 0.50568 val_ap= 0.69912 time= 0.11968\n",
      "Epoch: 0108 train_loss= 0.49383 val_ap= 0.69912 time= 0.10572\n",
      "Epoch: 0109 train_loss= 0.49470 val_ap= 0.68196 time= 0.11370\n",
      "Epoch: 0110 train_loss= 0.49378 val_ap= 0.65360 time= 0.12068\n",
      "Epoch: 0111 train_loss= 0.49668 val_ap= 0.66280 time= 0.14062\n",
      "Epoch: 0112 train_loss= 0.48927 val_ap= 0.66205 time= 0.12068\n",
      "Epoch: 0113 train_loss= 0.48841 val_ap= 0.68108 time= 0.11669\n",
      "Epoch: 0114 train_loss= 0.48753 val_ap= 0.67789 time= 0.10572\n",
      "Epoch: 0115 train_loss= 0.48564 val_ap= 0.68763 time= 0.10472\n",
      "Epoch: 0116 train_loss= 0.48541 val_ap= 0.70582 time= 0.10572\n",
      "Epoch: 0117 train_loss= 0.48162 val_ap= 0.69026 time= 0.11071\n",
      "Epoch: 0118 train_loss= 0.48419 val_ap= 0.69089 time= 0.12166\n",
      "Epoch: 0119 train_loss= 0.47993 val_ap= 0.68553 time= 0.11769\n",
      "Epoch: 0120 train_loss= 0.48525 val_ap= 0.68465 time= 0.11768\n",
      "Epoch: 0121 train_loss= 0.47979 val_ap= 0.69291 time= 0.10372\n",
      "Epoch: 0122 train_loss= 0.47906 val_ap= 0.66832 time= 0.10472\n",
      "Epoch: 0123 train_loss= 0.48809 val_ap= 0.69710 time= 0.11768\n",
      "Epoch: 0124 train_loss= 0.47956 val_ap= 0.68155 time= 0.11867\n",
      "Epoch: 0125 train_loss= 0.47388 val_ap= 0.68214 time= 0.11070\n",
      "Epoch: 0126 train_loss= 0.47246 val_ap= 0.70430 time= 0.11569\n",
      "Epoch: 0127 train_loss= 0.47108 val_ap= 0.69536 time= 0.10073\n",
      "Epoch: 0128 train_loss= 0.47049 val_ap= 0.68588 time= 0.10771\n",
      "Epoch: 0129 train_loss= 0.47516 val_ap= 0.69043 time= 0.12965\n",
      "Epoch: 0130 train_loss= 0.47059 val_ap= 0.68902 time= 0.10572\n",
      "Epoch: 0131 train_loss= 0.46810 val_ap= 0.69023 time= 0.10671\n",
      "Epoch: 0132 train_loss= 0.46906 val_ap= 0.68127 time= 0.12566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0133 train_loss= 0.46638 val_ap= 0.67260 time= 0.14262\n",
      "Epoch: 0134 train_loss= 0.47267 val_ap= 0.68709 time= 0.14162\n",
      "Epoch: 0135 train_loss= 0.46846 val_ap= 0.67940 time= 0.11968\n",
      "Epoch: 0136 train_loss= 0.47996 val_ap= 0.67504 time= 0.14262\n",
      "Epoch: 0137 train_loss= 0.46320 val_ap= 0.68959 time= 0.10871\n",
      "Epoch: 0138 train_loss= 0.46209 val_ap= 0.68631 time= 0.11368\n",
      "Epoch: 0139 train_loss= 0.46414 val_ap= 0.69383 time= 0.11868\n",
      "Epoch: 0140 train_loss= 0.46129 val_ap= 0.69959 time= 0.12267\n",
      "Epoch: 0141 train_loss= 0.46730 val_ap= 0.71059 time= 0.15459\n",
      "Epoch: 0142 train_loss= 0.46279 val_ap= 0.71610 time= 0.11768\n",
      "Epoch: 0143 train_loss= 0.46080 val_ap= 0.69592 time= 0.11270\n",
      "Epoch: 0144 train_loss= 0.46093 val_ap= 0.69356 time= 0.10472\n",
      "Epoch: 0145 train_loss= 0.45870 val_ap= 0.69042 time= 0.12367\n",
      "Epoch: 0146 train_loss= 0.46244 val_ap= 0.67858 time= 0.11868\n",
      "Epoch: 0147 train_loss= 0.46147 val_ap= 0.68068 time= 0.10472\n",
      "Epoch: 0148 train_loss= 0.45646 val_ap= 0.68373 time= 0.11769\n",
      "Epoch: 0149 train_loss= 0.45900 val_ap= 0.68748 time= 0.12566\n",
      "Epoch: 0150 train_loss= 0.45609 val_ap= 0.71379 time= 0.10273\n",
      "Epoch: 0151 train_loss= 0.46646 val_ap= 0.72161 time= 0.14162\n",
      "Epoch: 0152 train_loss= 0.45794 val_ap= 0.71568 time= 0.10971\n",
      "Epoch: 0153 train_loss= 0.45585 val_ap= 0.70380 time= 0.10372\n",
      "Epoch: 0154 train_loss= 0.45361 val_ap= 0.70462 time= 0.13264\n",
      "Epoch: 0155 train_loss= 0.45381 val_ap= 0.70669 time= 0.10372\n",
      "Epoch: 0156 train_loss= 0.45633 val_ap= 0.71623 time= 0.11569\n",
      "Epoch: 0157 train_loss= 0.45339 val_ap= 0.71519 time= 0.12067\n",
      "Epoch: 0158 train_loss= 0.45352 val_ap= 0.70776 time= 0.12566\n",
      "Epoch: 0159 train_loss= 0.45098 val_ap= 0.71328 time= 0.11868\n",
      "Epoch: 0160 train_loss= 0.45271 val_ap= 0.71328 time= 0.13663\n",
      "Epoch: 0161 train_loss= 0.45100 val_ap= 0.71349 time= 0.14561\n",
      "Epoch: 0162 train_loss= 0.44922 val_ap= 0.70507 time= 0.10971\n",
      "Epoch: 0163 train_loss= 0.44841 val_ap= 0.70128 time= 0.10572\n",
      "Epoch: 0164 train_loss= 0.45080 val_ap= 0.69420 time= 0.12068\n",
      "Epoch: 0165 train_loss= 0.44992 val_ap= 0.72019 time= 0.09674\n",
      "Epoch: 0166 train_loss= 0.45191 val_ap= 0.72825 time= 0.10671\n",
      "Epoch: 0167 train_loss= 0.44773 val_ap= 0.72754 time= 0.12866\n",
      "Epoch: 0168 train_loss= 0.44904 val_ap= 0.71800 time= 0.13763\n",
      "Epoch: 0169 train_loss= 0.44758 val_ap= 0.71250 time= 0.11669\n",
      "Epoch: 0170 train_loss= 0.45305 val_ap= 0.70335 time= 0.11070\n",
      "Epoch: 0171 train_loss= 0.44490 val_ap= 0.71284 time= 0.11270\n",
      "Epoch: 0172 train_loss= 0.44445 val_ap= 0.70918 time= 0.13464\n",
      "Epoch: 0173 train_loss= 0.44639 val_ap= 0.69700 time= 0.13763\n",
      "Epoch: 0174 train_loss= 0.44361 val_ap= 0.71739 time= 0.12566\n",
      "Epoch: 0175 train_loss= 0.44277 val_ap= 0.72639 time= 0.12267\n",
      "Epoch: 0176 train_loss= 0.44398 val_ap= 0.73298 time= 0.12167\n",
      "Epoch: 0177 train_loss= 0.44320 val_ap= 0.73526 time= 0.14561\n",
      "Epoch: 0178 train_loss= 0.44235 val_ap= 0.71524 time= 0.11868\n",
      "Epoch: 0179 train_loss= 0.44185 val_ap= 0.73753 time= 0.13364\n",
      "Epoch: 0180 train_loss= 0.44134 val_ap= 0.72408 time= 0.11370\n",
      "Epoch: 0181 train_loss= 0.44141 val_ap= 0.72243 time= 0.13065\n",
      "Epoch: 0182 train_loss= 0.44006 val_ap= 0.72549 time= 0.11769\n",
      "Epoch: 0183 train_loss= 0.43968 val_ap= 0.72549 time= 0.11769\n",
      "Epoch: 0184 train_loss= 0.44103 val_ap= 0.72549 time= 0.12865\n",
      "Epoch: 0185 train_loss= 0.44034 val_ap= 0.73753 time= 0.11769\n",
      "Epoch: 0186 train_loss= 0.44146 val_ap= 0.75176 time= 0.11769\n",
      "Epoch: 0187 train_loss= 0.44108 val_ap= 0.72578 time= 0.13065\n",
      "Epoch: 0188 train_loss= 0.43824 val_ap= 0.72902 time= 0.11569\n",
      "Epoch: 0189 train_loss= 0.43985 val_ap= 0.72982 time= 0.10671\n",
      "Epoch: 0190 train_loss= 0.43829 val_ap= 0.72982 time= 0.12068\n",
      "Epoch: 0191 train_loss= 0.43689 val_ap= 0.73445 time= 0.10771\n",
      "Epoch: 0192 train_loss= 0.43683 val_ap= 0.72909 time= 0.11968\n",
      "Epoch: 0193 train_loss= 0.43772 val_ap= 0.73097 time= 0.10869\n",
      "Epoch: 0194 train_loss= 0.44047 val_ap= 0.73365 time= 0.12068\n",
      "Epoch: 0195 train_loss= 0.43726 val_ap= 0.73365 time= 0.14760\n",
      "Epoch: 0196 train_loss= 0.43785 val_ap= 0.76008 time= 0.11669\n",
      "Epoch: 0197 train_loss= 0.43744 val_ap= 0.75841 time= 0.11469\n",
      "Epoch: 0198 train_loss= 0.43677 val_ap= 0.74629 time= 0.12965\n",
      "Epoch: 0199 train_loss= 0.43561 val_ap= 0.73720 time= 0.12167\n",
      "Epoch: 0200 train_loss= 0.43884 val_ap= 0.74105 time= 0.12566\n",
      "Epoch: 0201 train_loss= 0.43508 val_ap= 0.73030 time= 0.10871\n",
      "Epoch: 0202 train_loss= 0.43393 val_ap= 0.74105 time= 0.12666\n",
      "Epoch: 0203 train_loss= 0.43326 val_ap= 0.75320 time= 0.11370\n",
      "Epoch: 0204 train_loss= 0.43481 val_ap= 0.73030 time= 0.15259\n",
      "Epoch: 0205 train_loss= 0.43520 val_ap= 0.75936 time= 0.13265\n",
      "Epoch: 0206 train_loss= 0.43378 val_ap= 0.75320 time= 0.12666\n",
      "Epoch: 0207 train_loss= 0.43690 val_ap= 0.74245 time= 0.10572\n",
      "Epoch: 0208 train_loss= 0.43142 val_ap= 0.74843 time= 0.12866\n",
      "Epoch: 0209 train_loss= 0.43326 val_ap= 0.73849 time= 0.10273\n",
      "Epoch: 0210 train_loss= 0.43148 val_ap= 0.76388 time= 0.11667\n",
      "Epoch: 0211 train_loss= 0.43252 val_ap= 0.76010 time= 0.15060\n",
      "Epoch: 0212 train_loss= 0.43204 val_ap= 0.76388 time= 0.10472\n",
      "Epoch: 0213 train_loss= 0.43118 val_ap= 0.76388 time= 0.10671\n",
      "Epoch: 0214 train_loss= 0.43289 val_ap= 0.76676 time= 0.12367\n",
      "Epoch: 0215 train_loss= 0.43028 val_ap= 0.76443 time= 0.17752\n",
      "Epoch: 0216 train_loss= 0.43360 val_ap= 0.76425 time= 0.13464\n",
      "Epoch: 0217 train_loss= 0.43373 val_ap= 0.73892 time= 0.12267\n",
      "Epoch: 0218 train_loss= 0.43045 val_ap= 0.76643 time= 0.11968\n",
      "Epoch: 0219 train_loss= 0.43193 val_ap= 0.74944 time= 0.13265\n",
      "Epoch: 0220 train_loss= 0.42960 val_ap= 0.77076 time= 0.10073\n",
      "Epoch: 0221 train_loss= 0.42888 val_ap= 0.76611 time= 0.10572\n",
      "Epoch: 0222 train_loss= 0.42851 val_ap= 0.77364 time= 0.11269\n",
      "Epoch: 0223 train_loss= 0.42855 val_ap= 0.76425 time= 0.13165\n",
      "Epoch: 0224 train_loss= 0.43088 val_ap= 0.76376 time= 0.10173\n",
      "Epoch: 0225 train_loss= 0.42888 val_ap= 0.75869 time= 0.10771\n",
      "Epoch: 0226 train_loss= 0.42767 val_ap= 0.75772 time= 0.12965\n",
      "Epoch: 0227 train_loss= 0.43117 val_ap= 0.75869 time= 0.10771\n",
      "Epoch: 0228 train_loss= 0.42700 val_ap= 0.76303 time= 0.11569\n",
      "Epoch: 0229 train_loss= 0.42785 val_ap= 0.75581 time= 0.10472\n",
      "Epoch: 0230 train_loss= 0.42756 val_ap= 0.77114 time= 0.10970\n",
      "Epoch: 0231 train_loss= 0.42657 val_ap= 0.77021 time= 0.10572\n",
      "Epoch: 0232 train_loss= 0.42721 val_ap= 0.76392 time= 0.10971\n",
      "Epoch: 0233 train_loss= 0.42684 val_ap= 0.75241 time= 0.10871\n",
      "Epoch: 0234 train_loss= 0.42726 val_ap= 0.76643 time= 0.11968\n",
      "Epoch: 0235 train_loss= 0.42592 val_ap= 0.76494 time= 0.10671\n",
      "Epoch: 0236 train_loss= 0.42892 val_ap= 0.76061 time= 0.11070\n",
      "Epoch: 0237 train_loss= 0.42717 val_ap= 0.76456 time= 0.12367\n",
      "Epoch: 0238 train_loss= 0.42703 val_ap= 0.74490 time= 0.10273\n",
      "Epoch: 0239 train_loss= 0.42871 val_ap= 0.75937 time= 0.10871\n",
      "Epoch: 0240 train_loss= 0.42608 val_ap= 0.75268 time= 0.11569\n",
      "Epoch: 0241 train_loss= 0.42537 val_ap= 0.74673 time= 0.12965\n",
      "Epoch: 0242 train_loss= 0.42550 val_ap= 0.76558 time= 0.11369\n",
      "Epoch: 0243 train_loss= 0.42606 val_ap= 0.75127 time= 0.10871\n",
      "Epoch: 0244 train_loss= 0.42595 val_ap= 0.74787 time= 0.10971\n",
      "Epoch: 0245 train_loss= 0.42378 val_ap= 0.76466 time= 0.11170\n",
      "Epoch: 0246 train_loss= 0.42416 val_ap= 0.75259 time= 0.12566\n",
      "Epoch: 0247 train_loss= 0.42418 val_ap= 0.74607 time= 0.10572\n",
      "Epoch: 0248 train_loss= 0.42639 val_ap= 0.75349 time= 0.11868\n",
      "Epoch: 0249 train_loss= 0.42697 val_ap= 0.73834 time= 0.11270\n",
      "Epoch: 0250 train_loss= 0.42249 val_ap= 0.76125 time= 0.13065\n",
      "Epoch: 0251 train_loss= 0.42235 val_ap= 0.75228 time= 0.11868\n",
      "Epoch: 0252 train_loss= 0.42210 val_ap= 0.75457 time= 0.11968\n",
      "Epoch: 0253 train_loss= 0.42299 val_ap= 0.76886 time= 0.12168\n",
      "Epoch: 0254 train_loss= 0.42723 val_ap= 0.75457 time= 0.12965\n",
      "Epoch: 0255 train_loss= 0.42208 val_ap= 0.75998 time= 0.13265\n",
      "Epoch: 0256 train_loss= 0.42187 val_ap= 0.77226 time= 0.13264\n",
      "Epoch: 0257 train_loss= 0.42136 val_ap= 0.75202 time= 0.13863\n",
      "Epoch: 0258 train_loss= 0.42195 val_ap= 0.76326 time= 0.14960\n",
      "Epoch: 0259 train_loss= 0.42123 val_ap= 0.74540 time= 0.12666\n",
      "Epoch: 0260 train_loss= 0.42261 val_ap= 0.75202 time= 0.11070\n",
      "Epoch: 0261 train_loss= 0.42137 val_ap= 0.73672 time= 0.13464\n",
      "Epoch: 0262 train_loss= 0.42105 val_ap= 0.76342 time= 0.12566\n",
      "Epoch: 0263 train_loss= 0.42158 val_ap= 0.76500 time= 0.09973\n",
      "Epoch: 0264 train_loss= 0.42027 val_ap= 0.76220 time= 0.10472\n",
      "Epoch: 0265 train_loss= 0.41989 val_ap= 0.74434 time= 0.13863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0266 train_loss= 0.42344 val_ap= 0.76745 time= 0.10572\n",
      "Epoch: 0267 train_loss= 0.42132 val_ap= 0.74959 time= 0.12467\n",
      "Epoch: 0268 train_loss= 0.42129 val_ap= 0.74540 time= 0.12666\n",
      "Epoch: 0269 train_loss= 0.41981 val_ap= 0.74434 time= 0.14062\n",
      "Epoch: 0270 train_loss= 0.42301 val_ap= 0.73446 time= 0.13564\n",
      "Epoch: 0271 train_loss= 0.41941 val_ap= 0.73519 time= 0.13763\n",
      "Epoch: 0272 train_loss= 0.42170 val_ap= 0.74738 time= 0.12666\n",
      "Epoch: 0273 train_loss= 0.41920 val_ap= 0.76745 time= 0.11768\n",
      "Epoch: 0274 train_loss= 0.42092 val_ap= 0.75413 time= 0.10671\n",
      "Epoch: 0275 train_loss= 0.42313 val_ap= 0.76713 time= 0.09874\n",
      "Epoch: 0276 train_loss= 0.41912 val_ap= 0.75952 time= 0.12167\n",
      "Epoch: 0277 train_loss= 0.41951 val_ap= 0.74410 time= 0.12267\n",
      "Epoch: 0278 train_loss= 0.41853 val_ap= 0.75710 time= 0.11569\n",
      "Epoch: 0279 train_loss= 0.41925 val_ap= 0.75998 time= 0.14660\n",
      "Epoch: 0280 train_loss= 0.41869 val_ap= 0.74410 time= 0.11968\n",
      "Epoch: 0281 train_loss= 0.41785 val_ap= 0.74070 time= 0.11270\n",
      "Epoch: 0282 train_loss= 0.41898 val_ap= 0.75788 time= 0.11370\n",
      "Epoch: 0283 train_loss= 0.41872 val_ap= 0.75179 time= 0.13265\n",
      "Epoch: 0284 train_loss= 0.42147 val_ap= 0.74289 time= 0.10971\n",
      "Epoch: 0285 train_loss= 0.41829 val_ap= 0.77217 time= 0.10372\n",
      "Epoch: 0286 train_loss= 0.41732 val_ap= 0.76128 time= 0.10372\n",
      "Epoch: 0287 train_loss= 0.41806 val_ap= 0.77336 time= 0.12766\n",
      "Epoch: 0288 train_loss= 0.41844 val_ap= 0.74631 time= 0.10971\n",
      "Epoch: 0289 train_loss= 0.41850 val_ap= 0.76196 time= 0.10871\n",
      "Epoch: 0290 train_loss= 0.41815 val_ap= 0.74746 time= 0.10471\n",
      "Epoch: 0291 train_loss= 0.41750 val_ap= 0.75889 time= 0.10672\n",
      "Epoch: 0292 train_loss= 0.41687 val_ap= 0.76243 time= 0.11070\n",
      "Epoch: 0293 train_loss= 0.41711 val_ap= 0.77586 time= 0.11170\n",
      "Epoch: 0294 train_loss= 0.41811 val_ap= 0.77114 time= 0.10572\n",
      "Epoch: 0295 train_loss= 0.41739 val_ap= 0.75567 time= 0.11170\n",
      "Epoch: 0296 train_loss= 0.41631 val_ap= 0.78011 time= 0.13564\n",
      "Epoch: 0297 train_loss= 0.41661 val_ap= 0.78011 time= 0.13763\n",
      "Epoch: 0298 train_loss= 0.41733 val_ap= 0.76583 time= 0.11669\n",
      "Epoch: 0299 train_loss= 0.41619 val_ap= 0.75740 time= 0.10771\n",
      "Epoch: 0300 train_loss= 0.41767 val_ap= 0.73954 time= 0.12068\n",
      "Epoch: 0301 train_loss= 0.41675 val_ap= 0.75286 time= 0.13065\n",
      "Epoch: 0302 train_loss= 0.41551 val_ap= 0.77318 time= 0.11070\n",
      "Epoch: 0303 train_loss= 0.41630 val_ap= 0.78227 time= 0.13664\n",
      "Epoch: 0304 train_loss= 0.41641 val_ap= 0.78462 time= 0.12965\n",
      "Epoch: 0305 train_loss= 0.41611 val_ap= 0.75889 time= 0.10771\n",
      "Epoch: 0306 train_loss= 0.41608 val_ap= 0.73481 time= 0.10672\n",
      "Epoch: 0307 train_loss= 0.41519 val_ap= 0.75286 time= 0.12167\n",
      "Epoch: 0308 train_loss= 0.41511 val_ap= 0.75889 time= 0.14062\n",
      "Epoch: 0309 train_loss= 0.41551 val_ap= 0.75822 time= 0.09775\n",
      "Epoch: 0310 train_loss= 0.41538 val_ap= 0.77970 time= 0.11468\n",
      "Epoch: 0311 train_loss= 0.41500 val_ap= 0.77672 time= 0.11270\n",
      "Epoch: 0312 train_loss= 0.41446 val_ap= 0.77617 time= 0.10173\n",
      "Epoch: 0313 train_loss= 0.41928 val_ap= 0.75969 time= 0.11172\n",
      "Epoch: 0314 train_loss= 0.41616 val_ap= 0.79027 time= 0.10971\n",
      "Epoch: 0315 train_loss= 0.41906 val_ap= 0.76753 time= 0.11174\n",
      "Epoch: 0316 train_loss= 0.41503 val_ap= 0.77319 time= 0.10572\n",
      "Epoch: 0317 train_loss= 0.41419 val_ap= 0.77833 time= 0.11070\n",
      "Epoch: 0318 train_loss= 0.41460 val_ap= 0.77467 time= 0.12566\n",
      "Epoch: 0319 train_loss= 0.41629 val_ap= 0.77467 time= 0.10272\n",
      "Epoch: 0320 train_loss= 0.41466 val_ap= 0.77169 time= 0.12167\n",
      "Epoch: 0321 train_loss= 0.41403 val_ap= 0.76953 time= 0.11868\n",
      "Epoch: 0322 train_loss= 0.41364 val_ap= 0.76953 time= 0.12766\n",
      "Epoch: 0323 train_loss= 0.41472 val_ap= 0.78893 time= 0.10372\n",
      "Epoch: 0324 train_loss= 0.41538 val_ap= 0.78978 time= 0.10572\n",
      "Epoch: 0325 train_loss= 0.41525 val_ap= 0.79450 time= 0.10472\n",
      "Epoch: 0326 train_loss= 0.41393 val_ap= 0.77536 time= 0.09874\n",
      "Epoch: 0327 train_loss= 0.41605 val_ap= 0.76405 time= 0.10771\n",
      "Epoch: 0328 train_loss= 0.41644 val_ap= 0.76038 time= 0.10173\n",
      "Epoch: 0329 train_loss= 0.41346 val_ap= 0.77467 time= 0.10574\n",
      "Epoch: 0330 train_loss= 0.41321 val_ap= 0.76953 time= 0.10869\n",
      "Epoch: 0331 train_loss= 0.41387 val_ap= 0.76953 time= 0.10472\n",
      "Epoch: 0332 train_loss= 0.41298 val_ap= 0.77251 time= 0.10472\n",
      "Epoch: 0333 train_loss= 0.41289 val_ap= 0.78179 time= 0.10671\n",
      "Epoch: 0334 train_loss= 0.41689 val_ap= 0.78490 time= 0.11170\n",
      "Epoch: 0335 train_loss= 0.41346 val_ap= 0.78143 time= 0.13265\n",
      "Epoch: 0336 train_loss= 0.41457 val_ap= 0.77467 time= 0.09874\n",
      "Epoch: 0337 train_loss= 0.41234 val_ap= 0.77169 time= 0.12965\n",
      "Epoch: 0338 train_loss= 0.41245 val_ap= 0.77617 time= 0.14162\n",
      "Epoch: 0339 train_loss= 0.41281 val_ap= 0.78400 time= 0.11968\n",
      "Epoch: 0340 train_loss= 0.41366 val_ap= 0.77467 time= 0.12965\n",
      "Epoch: 0341 train_loss= 0.41425 val_ap= 0.77319 time= 0.10871\n",
      "Epoch: 0342 train_loss= 0.41235 val_ap= 0.78490 time= 0.12267\n",
      "Epoch: 0343 train_loss= 0.41222 val_ap= 0.77169 time= 0.14362\n",
      "Epoch: 0344 train_loss= 0.41369 val_ap= 0.77029 time= 0.12068\n",
      "Epoch: 0345 train_loss= 0.41139 val_ap= 0.78400 time= 0.11469\n",
      "Epoch: 0346 train_loss= 0.41724 val_ap= 0.78345 time= 0.12167\n",
      "Epoch: 0347 train_loss= 0.41335 val_ap= 0.78510 time= 0.14661\n",
      "Epoch: 0348 train_loss= 0.41156 val_ap= 0.78143 time= 0.10173\n",
      "Epoch: 0349 train_loss= 0.41125 val_ap= 0.78143 time= 0.11270\n",
      "Epoch: 0350 train_loss= 0.41182 val_ap= 0.78143 time= 0.11669\n",
      "Epoch: 0351 train_loss= 0.41123 val_ap= 0.77319 time= 0.14661\n",
      "Epoch: 0352 train_loss= 0.41095 val_ap= 0.78143 time= 0.10871\n",
      "Epoch: 0353 train_loss= 0.41072 val_ap= 0.78143 time= 0.12865\n",
      "Epoch: 0354 train_loss= 0.41084 val_ap= 0.78143 time= 0.14162\n",
      "Epoch: 0355 train_loss= 0.41114 val_ap= 0.78143 time= 0.11169\n",
      "Epoch: 0356 train_loss= 0.41205 val_ap= 0.76953 time= 0.10472\n",
      "Epoch: 0357 train_loss= 0.41032 val_ap= 0.78143 time= 0.13264\n",
      "Epoch: 0358 train_loss= 0.41080 val_ap= 0.78651 time= 0.10073\n",
      "Epoch: 0359 train_loss= 0.41102 val_ap= 0.78651 time= 0.12267\n",
      "Epoch: 0360 train_loss= 0.41173 val_ap= 0.78453 time= 0.13364\n",
      "Epoch: 0361 train_loss= 0.41044 val_ap= 0.78143 time= 0.11170\n",
      "Epoch: 0362 train_loss= 0.41128 val_ap= 0.78143 time= 0.10671\n",
      "Epoch: 0363 train_loss= 0.41096 val_ap= 0.79017 time= 0.14661\n",
      "Epoch: 0364 train_loss= 0.41009 val_ap= 0.78143 time= 0.10472\n",
      "Epoch: 0365 train_loss= 0.41139 val_ap= 0.78453 time= 0.11070\n",
      "Epoch: 0366 train_loss= 0.41000 val_ap= 0.78143 time= 0.13464\n",
      "Epoch: 0367 train_loss= 0.41160 val_ap= 0.78510 time= 0.12167\n",
      "Epoch: 0368 train_loss= 0.41134 val_ap= 0.77946 time= 0.13264\n",
      "Epoch: 0369 train_loss= 0.41184 val_ap= 0.78711 time= 0.10871\n",
      "Epoch: 0370 train_loss= 0.41060 val_ap= 0.78968 time= 0.11270\n",
      "Epoch: 0371 train_loss= 0.40980 val_ap= 0.79218 time= 0.11070\n",
      "Epoch: 0372 train_loss= 0.41069 val_ap= 0.78819 time= 0.11769\n",
      "Epoch: 0373 train_loss= 0.41063 val_ap= 0.78819 time= 0.12267\n",
      "Epoch: 0374 train_loss= 0.41081 val_ap= 0.78453 time= 0.13663\n",
      "Epoch: 0375 train_loss= 0.41058 val_ap= 0.78203 time= 0.12567\n",
      "Epoch: 0376 train_loss= 0.41074 val_ap= 0.77946 time= 0.10671\n",
      "Epoch: 0377 train_loss= 0.40932 val_ap= 0.78698 time= 0.10771\n",
      "Epoch: 0378 train_loss= 0.40965 val_ap= 0.78711 time= 0.10672\n",
      "Epoch: 0379 train_loss= 0.41153 val_ap= 0.77521 time= 0.11768\n",
      "Epoch: 0380 train_loss= 0.40973 val_ap= 0.78711 time= 0.11569\n",
      "Epoch: 0381 train_loss= 0.40897 val_ap= 0.78569 time= 0.14362\n",
      "Epoch: 0382 train_loss= 0.40984 val_ap= 0.78513 time= 0.12666\n",
      "Epoch: 0383 train_loss= 0.40904 val_ap= 0.78453 time= 0.11571\n",
      "Epoch: 0384 train_loss= 0.40899 val_ap= 0.78453 time= 0.13263\n",
      "Epoch: 0385 train_loss= 0.40959 val_ap= 0.78819 time= 0.10771\n",
      "Epoch: 0386 train_loss= 0.40914 val_ap= 0.79218 time= 0.11270\n",
      "Epoch: 0387 train_loss= 0.40926 val_ap= 0.78711 time= 0.11768\n",
      "Epoch: 0388 train_loss= 0.40851 val_ap= 0.78698 time= 0.11569\n",
      "Epoch: 0389 train_loss= 0.40877 val_ap= 0.78711 time= 0.10771\n",
      "Epoch: 0390 train_loss= 0.40919 val_ap= 0.78711 time= 0.11669\n",
      "Epoch: 0391 train_loss= 0.40938 val_ap= 0.78711 time= 0.10771\n",
      "Epoch: 0392 train_loss= 0.40843 val_ap= 0.79218 time= 0.11370\n",
      "Epoch: 0393 train_loss= 0.40840 val_ap= 0.79218 time= 0.11268\n",
      "Epoch: 0394 train_loss= 0.40935 val_ap= 0.78758 time= 0.14661\n",
      "Epoch: 0395 train_loss= 0.40839 val_ap= 0.79463 time= 0.10372\n",
      "Epoch: 0396 train_loss= 0.40844 val_ap= 0.78698 time= 0.13165\n",
      "Epoch: 0397 train_loss= 0.40776 val_ap= 0.79218 time= 0.10472\n",
      "Epoch: 0398 train_loss= 0.40806 val_ap= 0.79463 time= 0.10472\n",
      "Epoch: 0399 train_loss= 0.40762 val_ap= 0.79218 time= 0.11370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 train_loss= 0.40853 val_ap= 0.78711 time= 0.10971\n",
      "Epoch: 0401 train_loss= 0.40780 val_ap= 0.79218 time= 0.10971\n",
      "Epoch: 0402 train_loss= 0.40734 val_ap= 0.79157 time= 0.11569\n",
      "Epoch: 0403 train_loss= 0.40717 val_ap= 0.79463 time= 0.10472\n",
      "Epoch: 0404 train_loss= 0.40784 val_ap= 0.79938 time= 0.11868\n",
      "Epoch: 0405 train_loss= 0.40798 val_ap= 0.79829 time= 0.10871\n",
      "Epoch: 0406 train_loss= 0.40776 val_ap= 0.79829 time= 0.15060\n",
      "Epoch: 0407 train_loss= 0.40702 val_ap= 0.79218 time= 0.10272\n",
      "Epoch: 0408 train_loss= 0.40714 val_ap= 0.79463 time= 0.12965\n",
      "Epoch: 0409 train_loss= 0.40800 val_ap= 0.79157 time= 0.14461\n",
      "Epoch: 0410 train_loss= 0.40690 val_ap= 0.79463 time= 0.12666\n",
      "Epoch: 0411 train_loss= 0.40695 val_ap= 0.79463 time= 0.12367\n",
      "Epoch: 0412 train_loss= 0.40947 val_ap= 0.79077 time= 0.11669\n",
      "Epoch: 0413 train_loss= 0.40979 val_ap= 0.79334 time= 0.12566\n",
      "Epoch: 0414 train_loss= 0.40677 val_ap= 0.80483 time= 0.12965\n",
      "Epoch: 0415 train_loss= 0.40712 val_ap= 0.80483 time= 0.13265\n",
      "Epoch: 0416 train_loss= 0.40822 val_ap= 0.79996 time= 0.10771\n",
      "Epoch: 0417 train_loss= 0.40685 val_ap= 0.79463 time= 0.10971\n",
      "Epoch: 0418 train_loss= 0.40710 val_ap= 0.79157 time= 0.10871\n",
      "Epoch: 0419 train_loss= 0.40662 val_ap= 0.79463 time= 0.10273\n",
      "Epoch: 0420 train_loss= 0.40647 val_ap= 0.79463 time= 0.11170\n",
      "Epoch: 0421 train_loss= 0.40657 val_ap= 0.80483 time= 0.10173\n",
      "Epoch: 0422 train_loss= 0.40613 val_ap= 0.80177 time= 0.12167\n",
      "Epoch: 0423 train_loss= 0.40635 val_ap= 0.80483 time= 0.13564\n",
      "Epoch: 0424 train_loss= 0.40701 val_ap= 0.80483 time= 0.10372\n",
      "Epoch: 0425 train_loss= 0.40673 val_ap= 0.80483 time= 0.12367\n",
      "Epoch: 0426 train_loss= 0.40596 val_ap= 0.80177 time= 0.10472\n",
      "Epoch: 0427 train_loss= 0.40623 val_ap= 0.80177 time= 0.12068\n",
      "Epoch: 0428 train_loss= 0.40563 val_ap= 0.80483 time= 0.11669\n",
      "Epoch: 0429 train_loss= 0.40718 val_ap= 0.79996 time= 0.11569\n",
      "Epoch: 0430 train_loss= 0.40710 val_ap= 0.79996 time= 0.12068\n",
      "Epoch: 0431 train_loss= 0.40533 val_ap= 0.80483 time= 0.10672\n",
      "Epoch: 0432 train_loss= 0.40656 val_ap= 0.80239 time= 0.10871\n",
      "Epoch: 0433 train_loss= 0.40567 val_ap= 0.80483 time= 0.12467\n",
      "Epoch: 0434 train_loss= 0.40528 val_ap= 0.80483 time= 0.12466\n",
      "Epoch: 0435 train_loss= 0.40567 val_ap= 0.79682 time= 0.14561\n",
      "Epoch: 0436 train_loss= 0.40506 val_ap= 0.80483 time= 0.10971\n",
      "Epoch: 0437 train_loss= 0.40518 val_ap= 0.80483 time= 0.12566\n",
      "Epoch: 0438 train_loss= 0.40525 val_ap= 0.80483 time= 0.11868\n",
      "Epoch: 0439 train_loss= 0.40493 val_ap= 0.80483 time= 0.10572\n",
      "Epoch: 0440 train_loss= 0.40540 val_ap= 0.80483 time= 0.11569\n",
      "Epoch: 0441 train_loss= 0.40490 val_ap= 0.80483 time= 0.14561\n",
      "Epoch: 0442 train_loss= 0.40484 val_ap= 0.79682 time= 0.15359\n",
      "Epoch: 0443 train_loss= 0.40501 val_ap= 0.79682 time= 0.10572\n",
      "Epoch: 0444 train_loss= 0.40492 val_ap= 0.80483 time= 0.13663\n",
      "Epoch: 0445 train_loss= 0.40519 val_ap= 0.80177 time= 0.13065\n",
      "Epoch: 0446 train_loss= 0.40499 val_ap= 0.80483 time= 0.12367\n",
      "Epoch: 0447 train_loss= 0.40559 val_ap= 0.79675 time= 0.10971\n",
      "Epoch: 0448 train_loss= 0.40472 val_ap= 0.80302 time= 0.12068\n",
      "Epoch: 0449 train_loss= 0.40490 val_ap= 0.79682 time= 0.11669\n",
      "Epoch: 0450 train_loss= 0.40449 val_ap= 0.80302 time= 0.11370\n",
      "Epoch: 0451 train_loss= 0.40423 val_ap= 0.80302 time= 0.13465\n",
      "Epoch: 0452 train_loss= 0.40554 val_ap= 0.79829 time= 0.14360\n",
      "Epoch: 0453 train_loss= 0.40399 val_ap= 0.80483 time= 0.13165\n",
      "Epoch: 0454 train_loss= 0.40456 val_ap= 0.80483 time= 0.11569\n",
      "Epoch: 0455 train_loss= 0.40593 val_ap= 0.80483 time= 0.12367\n",
      "Epoch: 0456 train_loss= 0.40485 val_ap= 0.80177 time= 0.13564\n",
      "Epoch: 0457 train_loss= 0.40435 val_ap= 0.79982 time= 0.12666\n",
      "Epoch: 0458 train_loss= 0.40402 val_ap= 0.79982 time= 0.12866\n",
      "Epoch: 0459 train_loss= 0.40373 val_ap= 0.80135 time= 0.13564\n",
      "Epoch: 0460 train_loss= 0.40467 val_ap= 0.79675 time= 0.10871\n",
      "Epoch: 0461 train_loss= 0.40395 val_ap= 0.80135 time= 0.13065\n",
      "Epoch: 0462 train_loss= 0.40400 val_ap= 0.79829 time= 0.12167\n",
      "Epoch: 0463 train_loss= 0.40433 val_ap= 0.80196 time= 0.12566\n",
      "Epoch: 0464 train_loss= 0.40431 val_ap= 0.80302 time= 0.14062\n",
      "Epoch: 0465 train_loss= 0.40373 val_ap= 0.80135 time= 0.13364\n",
      "Epoch: 0466 train_loss= 0.40406 val_ap= 0.79515 time= 0.10971\n",
      "Epoch: 0467 train_loss= 0.40557 val_ap= 0.80302 time= 0.12167\n",
      "Epoch: 0468 train_loss= 0.40371 val_ap= 0.79675 time= 0.13963\n",
      "Epoch: 0469 train_loss= 0.40402 val_ap= 0.80135 time= 0.14461\n",
      "Epoch: 0470 train_loss= 0.40344 val_ap= 0.79675 time= 0.13265\n",
      "Epoch: 0471 train_loss= 0.40398 val_ap= 0.80135 time= 0.12167\n",
      "Epoch: 0472 train_loss= 0.40308 val_ap= 0.79675 time= 0.12367\n",
      "Epoch: 0473 train_loss= 0.40440 val_ap= 0.80042 time= 0.11669\n",
      "Epoch: 0474 train_loss= 0.40401 val_ap= 0.79675 time= 0.13564\n",
      "Epoch: 0475 train_loss= 0.40293 val_ap= 0.80135 time= 0.10372\n",
      "Epoch: 0476 train_loss= 0.40334 val_ap= 0.80135 time= 0.11170\n",
      "Epoch: 0477 train_loss= 0.40294 val_ap= 0.79515 time= 0.12267\n",
      "Epoch: 0478 train_loss= 0.40311 val_ap= 0.79982 time= 0.13663\n",
      "Epoch: 0479 train_loss= 0.40343 val_ap= 0.79675 time= 0.10173\n",
      "Epoch: 0480 train_loss= 0.40287 val_ap= 0.79982 time= 0.10571\n",
      "Epoch: 0481 train_loss= 0.40289 val_ap= 0.80042 time= 0.14262\n",
      "Epoch: 0482 train_loss= 0.40376 val_ap= 0.80042 time= 0.14262\n",
      "Epoch: 0483 train_loss= 0.40238 val_ap= 0.79982 time= 0.12866\n",
      "Epoch: 0484 train_loss= 0.40280 val_ap= 0.79675 time= 0.12866\n",
      "Epoch: 0485 train_loss= 0.40290 val_ap= 0.80135 time= 0.12666\n",
      "Epoch: 0486 train_loss= 0.40225 val_ap= 0.80135 time= 0.12766\n",
      "Epoch: 0487 train_loss= 0.40274 val_ap= 0.79829 time= 0.10871\n",
      "Epoch: 0488 train_loss= 0.40276 val_ap= 0.79982 time= 0.09973\n",
      "Epoch: 0489 train_loss= 0.40285 val_ap= 0.79361 time= 0.11170\n",
      "Epoch: 0490 train_loss= 0.40234 val_ap= 0.79361 time= 0.11567\n",
      "Epoch: 0491 train_loss= 0.40212 val_ap= 0.79982 time= 0.13863\n",
      "Epoch: 0492 train_loss= 0.40221 val_ap= 0.79675 time= 0.10173\n",
      "Epoch: 0493 train_loss= 0.40307 val_ap= 0.79982 time= 0.12267\n",
      "Epoch: 0494 train_loss= 0.40246 val_ap= 0.79675 time= 0.10572\n",
      "Epoch: 0495 train_loss= 0.40198 val_ap= 0.79675 time= 0.12467\n",
      "Epoch: 0496 train_loss= 0.40183 val_ap= 0.79982 time= 0.10671\n",
      "Epoch: 0497 train_loss= 0.40169 val_ap= 0.79431 time= 0.11569\n",
      "Epoch: 0498 train_loss= 0.40160 val_ap= 0.79982 time= 0.12367\n",
      "Epoch: 0499 train_loss= 0.40228 val_ap= 0.79675 time= 0.13763\n",
      "Epoch: 0500 train_loss= 0.40162 val_ap= 0.79675 time= 0.11769\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.7521947789930438\n",
      "Test AP score: 0.767001452685127\n"
     ]
    }
   ],
   "source": [
    "# if once = False, run the model for 10 times with different random seeds.\n",
    "# if plot = True, then plot the learning curve every 10 epochs.\n",
    "once = True\n",
    "plot = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        GNP(args,plot)\n",
    "    else:\n",
    "        test_roc = []\n",
    "        test_ap = []\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            roc_score, ap_score = GNP(args,plot)\n",
    "            test_roc.append(roc_score)\n",
    "            test_ap.append(ap_score)\n",
    "        print(test_roc)\n",
    "        print('mean test AUC is',np.mean(test_roc),' std ', np.std(test_roc))\n",
    "        print(test_ap)\n",
    "        print('mean test AP is ',np.mean(test_ap), ' std ', np.std(test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
