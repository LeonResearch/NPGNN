{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from model import GNP_Encoder, GNP_Decoder, InnerProductDecoder\n",
    "from optimizer import loss_function3\n",
    "from utils import load_data, mask_test_edges, preprocess_graph, get_roc_score, ct_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29c99a2cb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, default='gcn_vae', help=\"models used\")\n",
    "parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=400, help='Number of epochs to train.')\n",
    "parser.add_argument('--hiddenEnc', type=int, default=32, help='Number of units in hidden layer of Encoder.')\n",
    "parser.add_argument('--z_dim', type=int, default=16, help='Dimension of latent code Z.')\n",
    "parser.add_argument('--hiddenDec', type=int, default=64, help='Number of units in hidden layer of Decoder.')\n",
    "parser.add_argument('--outDimDec', type=int, default=16, help='Output Dimension of the Decoder.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset_str', type=str, default='citeseer', help='type of dataset.')\n",
    "parser.add_argument('--n_z_samples', type=int, default=1, help='Number of Z samples')\n",
    "\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, std, n):\n",
    "    \"\"\"Reparameterisation trick.\"\"\"\n",
    "    eps = torch.autograd.Variable(std.data.new(n,args.z_dim).normal_())\n",
    "    return mu + std * eps \n",
    "\n",
    "def KLD_gaussian(mu_q, std_q, mu_p, std_p):\n",
    "    \"\"\"Analytical KLD between 2 Gaussians.\"\"\"\n",
    "    qs2 = std_q**2 + 1e-16\n",
    "    ps2 = std_p**2 + 1e-16\n",
    "    \n",
    "    return (qs2/ps2 + ((mu_q-mu_p)**2)/ps2 + torch.log(ps2/qs2) - 1.0).sum()*0.5\n",
    "    \n",
    "def NPGNN(args,if_plot):\n",
    "    adj, features = load_data(args.dataset_str)\n",
    "    n_nodes, feat_dim, = features.shape\n",
    "    features = features.to(device)\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))\n",
    "    \n",
    "    \n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    adj_deep_copy = adj\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)   \n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    encoder = GNP_Encoder(feat_dim, args.hiddenEnc, args.z_dim, args.dropout)\n",
    "    decoder = GNP_Decoder(feat_dim + args.z_dim, args.hiddenDec, args.outDimDec, args.dropout)\n",
    "    innerDecoder = InnerProductDecoder(args.dropout, act=lambda x: x)\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    innerDecoder.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(decoder.parameters())+list(encoder.parameters()), args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_ap = []\n",
    "    t = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        innerDecoder.train()\n",
    "        \n",
    "        \n",
    "        np.random.seed(epoch)\n",
    "        adj_context  = ct_split(adj_deep_copy )\n",
    "        adj_context_norm = preprocess_graph(adj_context)\n",
    "        adj_context_norm = adj_context_norm.to(device)\n",
    "        \n",
    "        \n",
    "        c_z_mu, c_z_logvar = encoder(features, adj_context_norm)\n",
    "        ct_z_mu, ct_z_logvar = encoder(features, adj_norm)\n",
    "        \n",
    "        #Sample a batch of zs using reparam trick for MC estimation\n",
    "        zs = sample_z(ct_z_mu, torch.exp(ct_z_logvar), args.n_z_samples)\n",
    "        zs = zs.to(device)   \n",
    "\n",
    "        # Get the predictive distribution of y*\n",
    "        mu, std = decoder(features, zs)\n",
    "\n",
    "        emb = torch.mean(mu, dim = 1)\n",
    "        pred_adj = innerDecoder(emb)\n",
    "        \n",
    "        #Compute loss and backprop\n",
    "        loss = loss_function3(preds=pred_adj, labels=adj_label, norm=norm, pos_weight=torch.tensor(pos_weight))  + KLD_gaussian(ct_z_mu, torch.exp(ct_z_logvar), c_z_mu, torch.exp(c_z_logvar))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%49 == 0:\n",
    "            hidden_emb = emb.cpu().data.numpy()\n",
    "            roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "            train_loss.append(cur_loss)\n",
    "            val_ap.append(ap_curr)\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "                  \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "                  )\n",
    "            t = time.time()\n",
    "        \n",
    "        \n",
    "\n",
    "        if (if_plot == True) and (epoch%49 == 0):\n",
    "            fig = plt.figure(figsize=(30,10))\n",
    "            ax1 = fig.add_subplot(1,2,1)\n",
    "            ax2 = fig.add_subplot(1,2,2)\n",
    "            \n",
    "            ax1.plot(train_loss, label='Training loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend(frameon=False)\n",
    "    \n",
    "            ax2.plot(val_ap, label='Validation Average Precision Score',color='Red')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('AP')\n",
    "            ax2.legend(frameon=False)\n",
    "            \n",
    "            plt.show()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using citeseer dataset\n",
      "Epoch: 0001 train_loss= 2.21036 val_ap= 0.49238 time= 0.52253\n",
      "Epoch: 0050 train_loss= 0.55814 val_ap= 0.84373 time= 2.12432\n",
      "Epoch: 0099 train_loss= 0.45683 val_ap= 0.93524 time= 2.13831\n",
      "Epoch: 0148 train_loss= 0.43771 val_ap= 0.94259 time= 2.04569\n",
      "Epoch: 0197 train_loss= 0.42768 val_ap= 0.94740 time= 2.04547\n",
      "Epoch: 0246 train_loss= 0.42132 val_ap= 0.95128 time= 2.01942\n",
      "Epoch: 0295 train_loss= 0.41760 val_ap= 0.95232 time= 2.02658\n",
      "Epoch: 0344 train_loss= 0.41500 val_ap= 0.95323 time= 2.09440\n",
      "Epoch: 0393 train_loss= 0.41306 val_ap= 0.95278 time= 2.07345\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9480545827798574\n",
      "Test AP score: 0.9546358417879327\n"
     ]
    }
   ],
   "source": [
    "# if once = False, run the model for 10 times with different random seeds.\n",
    "# if plot = True, then plot the learning curve every 10 epochs.\n",
    "once = True\n",
    "plot = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        NPGNN(args, plot)\n",
    "    else:\n",
    "        test_roc = []\n",
    "        test_ap = []\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            roc_score, ap_score = NPGNN(args,plot)\n",
    "            test_roc.append(roc_score)\n",
    "            test_ap.append(ap_score)\n",
    "        print(test_roc)\n",
    "        print('mean test AUC is',np.mean(test_roc),' std ', np.std(test_roc))\n",
    "        print(test_ap)\n",
    "        print('mean test AP is ',np.mean(test_ap), ' std ', np.std(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
