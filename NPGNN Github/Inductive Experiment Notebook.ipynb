{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from model import GNP_Encoder, GNP_Decoder, InnerProductDecoder\n",
    "from optimizer import loss_function3\n",
    "from utils import load_data, preprocess_graph, get_roc_score, train_val_test_split, ct_split_nodes\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17ebafd3b30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=400, help='Number of epochs to train.')\n",
    "parser.add_argument('--hiddenEnc', type=int, default=32, help='Number of units in hidden layer of Encoder.')\n",
    "parser.add_argument('--z_dim', type=int, default=16, help='Dimension of latent code Z.')\n",
    "parser.add_argument('--hiddenDec', type=int, default=64, help='Number of units in hidden layer of Decoder.')\n",
    "parser.add_argument('--outDimDec', type=int, default=16, help='Output Dimension of the Decoder.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset_str', type=str, default='citeseer', help='type of dataset.')\n",
    "parser.add_argument('--n_z_samples', type=int, default=1, help='Number of Z samples')\n",
    "\n",
    "\n",
    "args,unknown = parser.parse_known_args()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(mu, std, n):\n",
    "    \"\"\"Reparameterisation trick.\"\"\"\n",
    "    eps = torch.autograd.Variable(std.data.new(n,args.z_dim).normal_())\n",
    "    return mu + std * eps \n",
    "\n",
    "def KLD_gaussian(mu_q, std_q, mu_p, std_p):\n",
    "    \"\"\"Analytical KLD between 2 Gaussians.\"\"\"\n",
    "    qs2 = std_q**2 + 1e-16\n",
    "    ps2 = std_p**2 + 1e-16\n",
    "    \n",
    "    return (qs2/ps2 + ((mu_q-mu_p)**2)/ps2 + torch.log(ps2/qs2) - 1.0).sum()*0.5\n",
    "    \n",
    "def NPGNN(args,if_plot):\n",
    "    adj, features = load_data(args.dataset_str)\n",
    "    n_nodes, feat_dim, = features.shape\n",
    "    features = features.to(device)\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))   \n",
    "    \n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # You can change the proportion of nodes for training and testing for different few-shot inductive tasks.\n",
    "    # Change testing nodes proportion in train_val_test_split from utils.py \n",
    "    # This notebook use an example of training with 30% nodes and around 10% links, \n",
    "    # to predict the rest 90% links in testing.\n",
    "    features_train, adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false \\\n",
    "    = train_val_test_split(features,adj)\n",
    "    adj = adj_train\n",
    "    adj_deep_copy = adj\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    \n",
    "    adj_label = adj + sp.eye(adj.shape[0])\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    adj_label = adj_label.to(device)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    encoder = GNP_Encoder(feat_dim, args.hiddenEnc, args.z_dim, args.dropout)\n",
    "    decoder = GNP_Decoder(feat_dim + args.z_dim, args.hiddenDec, args.outDimDec, args.dropout)\n",
    "    innerDecoder = InnerProductDecoder(args.dropout, act=lambda x: x)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    innerDecoder.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(decoder.parameters())+list(encoder.parameters()), args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_ap = []\n",
    "    t = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        innerDecoder.train()\n",
    "        \n",
    "        \n",
    "        np.random.seed(epoch)\n",
    "        features_context, adj_context  = ct_split_nodes(features_train, adj_deep_copy )\n",
    "        adj_context_norm = preprocess_graph(adj_context)\n",
    "        adj_context_norm = adj_context_norm.to(device) \n",
    "        \n",
    "        c_z_mu, c_z_logvar = encoder(features_context, adj_context_norm)\n",
    "        ct_z_mu, ct_z_logvar = encoder(features_train, adj_norm)\n",
    "        \n",
    "        #Sample a batch of zs using reparam trick.\n",
    "        zs = sample_z(ct_z_mu, torch.exp(ct_z_logvar), args.n_z_samples)\n",
    "        zs = zs.to(device)\n",
    "        \n",
    "        # Get the predictive distribution of y*\n",
    "        mu, std = decoder(features_train, zs)\n",
    "\n",
    "        emb = torch.mean(mu, dim = 1)\n",
    "        pred_adj = innerDecoder(emb)\n",
    "        \n",
    "        #Compute loss and backprop\n",
    "        loss = loss_function3(preds=pred_adj, labels=adj_label, norm=norm, pos_weight=torch.tensor(pos_weight)) + KLD_gaussian(ct_z_mu, torch.exp(ct_z_logvar), c_z_mu, torch.exp(c_z_logvar))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%19 == 0:        \n",
    "            decoder.eval()\n",
    "            mu, std = decoder(features, zs)\n",
    "            emb = torch.mean(mu, dim = 1)\n",
    "            hidden_emb = emb.cpu().data.numpy()\n",
    "            roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "            train_loss.append(cur_loss)\n",
    "            val_ap.append(ap_curr)\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "                  \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            t = time.time()\n",
    "        \n",
    "        if (if_plot == True) and (epoch%10 == 0):\n",
    "            fig = plt.figure(figsize=(30,10))\n",
    "            ax1 = fig.add_subplot(1,2,1)\n",
    "            ax2 = fig.add_subplot(1,2,2)\n",
    "            \n",
    "            ax1.plot(train_loss, label='Training loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend(frameon=False)\n",
    "    \n",
    "            ax2.plot(val_ap, label='Validation Average Precision Score',color='Red')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('AP')\n",
    "            ax2.legend(frameon=False)\n",
    "            \n",
    "            plt.show()    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    return roc_score, ap_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using citeseer dataset\n",
      "number of training edges: 3855\n",
      "number of total edges: 4552\n",
      "training edges/total edges: 0.8468804920913884\n",
      "Epoch: 0001 train_loss= 2.21067 val_ap= 0.61941 time= 2.61572\n",
      "Epoch: 0020 train_loss= 0.77462 val_ap= 0.74007 time= 40.01886\n",
      "Epoch: 0039 train_loss= 0.63013 val_ap= 0.84761 time= 39.92859\n",
      "Epoch: 0058 train_loss= 0.52169 val_ap= 0.92229 time= 39.20084\n",
      "Epoch: 0077 train_loss= 0.48297 val_ap= 0.92837 time= 41.23771\n",
      "Epoch: 0096 train_loss= 0.46567 val_ap= 0.92489 time= 41.76829\n",
      "Epoch: 0115 train_loss= 0.44809 val_ap= 0.92707 time= 42.32794\n",
      "Epoch: 0134 train_loss= 0.44442 val_ap= 0.92787 time= 41.06981\n",
      "Epoch: 0153 train_loss= 0.43625 val_ap= 0.92756 time= 41.25296\n",
      "Epoch: 0172 train_loss= 0.43462 val_ap= 0.92883 time= 41.53292\n",
      "Epoch: 0191 train_loss= 0.43038 val_ap= 0.93125 time= 41.91390\n",
      "Epoch: 0210 train_loss= 0.43461 val_ap= 0.92568 time= 42.46313\n",
      "Epoch: 0229 train_loss= 0.42687 val_ap= 0.93181 time= 41.08811\n",
      "Epoch: 0248 train_loss= 0.42192 val_ap= 0.93036 time= 40.30620\n",
      "Epoch: 0267 train_loss= 0.41931 val_ap= 0.93007 time= 42.84587\n",
      "Epoch: 0286 train_loss= 0.41856 val_ap= 0.93084 time= 41.35468\n",
      "Epoch: 0305 train_loss= 0.41879 val_ap= 0.92897 time= 40.82581\n",
      "Epoch: 0324 train_loss= 0.41577 val_ap= 0.93057 time= 40.49016\n",
      "Epoch: 0343 train_loss= 0.41695 val_ap= 0.93159 time= 40.07041\n",
      "Epoch: 0362 train_loss= 0.41521 val_ap= 0.93024 time= 39.41270\n",
      "Epoch: 0381 train_loss= 0.41371 val_ap= 0.93109 time= 39.87635\n",
      "Epoch: 0400 train_loss= 0.41321 val_ap= 0.93160 time= 39.73098\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9016882019079822\n",
      "Test AP score: 0.9120690261725374\n"
     ]
    }
   ],
   "source": [
    "# if once = False, run the model for 10 times with different random seeds.\n",
    "# if plot = True, then plot the learning curve every 10 epochs.\n",
    "once = True\n",
    "plot = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if __name__ == '__main__':\n",
    "    if once == True:\n",
    "        NPGNN(args,plot)\n",
    "    else:\n",
    "        test_roc = []\n",
    "        test_ap = []\n",
    "        for seed in range(10):\n",
    "            print('Seed',seed)\n",
    "            args.seed = seed\n",
    "            torch.manual_seed(args.seed)\n",
    "            roc_score, ap_score = NPGNN(args,plot)\n",
    "            test_roc.append(roc_score)\n",
    "            test_ap.append(ap_score)\n",
    "        print(test_roc)\n",
    "        print('mean test AUC is',np.mean(test_roc),' std ', np.std(test_roc))\n",
    "        print(test_ap)\n",
    "        print('mean test AP is ',np.mean(test_ap), ' std ', np.std(test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
